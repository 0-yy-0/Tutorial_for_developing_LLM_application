{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、调用百度文心"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 文心一言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文心一言，由百度于2023年3月27日推出的中文大模型，是目前国内大语言模型的代表产品。受限于中文语料质量差异及国内计算资源、计算技术瓶颈，文心一言在整体性能上距离 ChatGPT 仍有一定差异，但在中文语境下已展现出了较为优越的性能。文心一言所考虑的落地场景包括多模态生成、文学创作等多种商业场景，其目标是在中文语境下赶超 ChatGPT。当然，要真正战胜 ChatGPT，百度还有很长的路要走；但在生成式 AI 监管较为严格的国内，作为第一批被允许向公众开放的生成式 AI 应用，文心一言相对无法被公开使用的 ChatGPT 还是具备一定商业上的优势。\n",
    "\n",
    "百度同样提供了文心一言的 API 接口，其于推出大模型的同时推出了文心千帆企业级大语言模型服务平台，包括了百度整套大语言模型开发工作链。对于不具备大模型实际落地能力的中小企业或传统企业，考虑文心千帆是一个可行的选择。当然，本教程仅包括通过文心千帆平台调用文心一言 API，对于其他企业级服务不予讨论。不过，值得额外注意的事，国内大模型厂商对于 API 的服务相对疏忽，文心一言 API 的实际性能与其 Web 应用所展现出来的能力还存在一定差异。\n",
    "\n",
    "在本章节中，我们同样将讲述两种通过 Python 代码调用百度文心一言大模型的方法：直接调用百度文心原生接口；使用 LangChain 调用百度文心接口。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 获取文心一言调用秘钥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样，要调用文心一言 API，需要先获取文心一言调用秘钥，在代码中需要配置自己的秘钥才能实现对模型的调用。\n",
    "\n",
    "百度文心有两层秘钥认证，第一层是拥有调用权限的账户可以从账户中获取的 API_Key 和 Secret_Key，每一个账户可以创建若干个应用，每个应用会对应一个 API_Key 和 Secret_Key。\n",
    "\n",
    "在获取完成 API_Key 和 Secret_Key 后，还需要基于这两个 Key 去获取 access_token 值。access_token 是第二层认证，基于 access_token 即可调用百度文心大模型，而 access_token 是可以控制基于时间或流量过期的。通过上述两层秘钥认证，可以进一步提高企业服务的安全性。\n",
    "\n",
    "当然，在本教程中，我们并不需要将两层秘钥分离。我们将在该部分简述如何获取 API_Key、Secret_Key 以及如何基于 Key 获取 access_token 值，后续我们将直接通过 access_token 来调用文心大模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../figures/baidu_qianfan_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们需要进入[文心千帆服务平台](https://console.bce.baidu.com/qianfan/overview)，点击上述应用接入按钮，创建一个调用文心大模型的应用。注意，你需要首先有一个经过实名认证的百度账号。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../figures/baidu_qianfan_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着点击“去创建”按钮，进入应用创建界面："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../figures/baidu_qianfan_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单输入基本信息，选择默认配置，创建应用即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../figures/baidu_qianfan_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建完成后，我们可以在控制台看到创建的应用的 `AppID`、`API Key`、`Secret Key`。使用这里的 `API Key` 和 `Secret Key` 即可进行 access_token 的获取。\n",
    "\n",
    "access_token 需要通过代码 post 访问指定网址得到："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_access_token():\n",
    "    \"\"\"\n",
    "    使用 API Key，Secret Key 获取access_token，替换下列示例中的应用API Key、应用Secret Key\n",
    "    \"\"\"\n",
    "    # 指定网址\n",
    "    url = \"https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id={api_key}&client_secret={secret_key}\"\n",
    "    # 设置 POST 访问\n",
    "    payload = json.dumps(\"\")\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    # 通过 POST 访问获取账户对应的 access_token\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    return response.json().get(\"access_token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上述代码，即可获取到账户对应的 access_token，后续使用 access_token 即可调用百度文心大模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 调用百度文心原生接口"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在完成 access_token 的获取后，可以同样通过 POST 访问来调用百度文心原生接口："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wenxin(prompt):\n",
    "    # 调用接口\n",
    "    url = \"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/eb-instant?access_token={access_token}\"\n",
    "    # 配置 POST 参数\n",
    "    payload = json.dumps({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",# user prompt\n",
    "                \"content\": \"{}\".format(prompt)# 输入的 prompt\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    # 发起请求\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    # 返回的是一个 Json 字符串\n",
    "    js = json.loads(response.text)\n",
    "    print(js[\"result\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "百度文心的 Prompt 格式同 OpenAI 的 Prompt 格式相似，但文心并没有提供 system prompt 级别的配置，仅支持 user、assistant 两个级别，可以在传入参数的 message 中配置。我们此处仅使用 user prompt 来实现调用。\n",
    "\n",
    "同时，百度文心返回的数据也是一个 Json 字符串，我们可以调用其中的 result 属性来获取返回数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您好！我是百度研发的知识增强大语言模型，中文名是文心一言，英文名是ERNIE Bot。我能够与人对话互动，回答问题，协助创作，高效便捷地帮助人们获取信息、知识和灵感。\n"
     ]
    }
   ],
   "source": [
    "get_wenxin(\"你好\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "百度千帆提供了多种模型接口供调用，此处我们主要使用 `ERNIE-Bot-turbo` 模型的 chat 接口，也就是常说的百度文心大模型。此处简要介绍文心大模型接口的常用参数：\n",
    "\n",
    "    · messages，即调用的 Prompt。文心的 messages 配置与 ChatGPT 有一定区别，其不支持 max_token 参数，由模型自行控制最大 token 数，content 总长度不能超过11200字符，否则模型就会自行对前文依次遗忘。文心的 messages 有以下几点要求：① 一个成员为单轮对话，多个成员为多轮对话；② 最后一个 message 为当前对话，前面的 message 为历史对话；③ 必须为奇数个对象，message 中的 role 必须依次是 user、assistant。\n",
    "\n",
    "    · stream，是否使用流式传输。\n",
    "\n",
    "    · temperature：温度系数，默认0.95，文心的 temperature 参数要求范围在0~1之间，不能设置为0。\n",
    "\n",
    "我们同样封装一个调用百度文心大模型的函数供之后使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个封装 Wenxin 接口的函数，参数为 Prompt，返回对应结果\n",
    "def get_completion_weixin(prompt, temperature = 0.1, access_token = \"\"):\n",
    "    '''\n",
    "    prompt: 对应的提示词\n",
    "    temperature：温度系数\n",
    "    access_token：已获取到的秘钥\n",
    "    '''\n",
    "    url = f\"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/eb-instant?access_token={access_token}\"\n",
    "    # 配置 POST 参数\n",
    "    payload = json.dumps({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",# user prompt\n",
    "                \"content\": \"{}\".format(prompt)# 输入的 prompt\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\" : temperature\n",
    "    })\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    # 发起请求\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    # 返回的是一个 Json 字符串\n",
    "    js = json.loads(response.text)\n",
    "    # print(js)\n",
    "    return js[\"result\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'您好！我是百度研发的知识增强大语言模型，中文名是文心一言，英文名是ERNIE Bot。我能够与人对话互动，回答问题，协助创作，高效便捷地帮助人们获取信息、知识和灵感。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"你好\"\n",
    "access_token = \"xxx\"\n",
    "get_completion_weixin(prompt, access_token=access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 使用 LangChain 调用百度文心"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们同样可以通过 LangChain 框架来调用百度文心大模型，以将文心模型接入到我们的应用框架中。\n",
    "\n",
    "但是，原生的 LangChain 是不支持文心调用的，我们需要安装文心的 LangChain 插件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install langchain-wenxin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain-wenxin 封装了两层秘钥认证的细节，我们可以直接全局配置文心的两个秘钥即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export BAIDU_API_KEY=\"<你的百度api key>\"                            \n",
    "!export BAIDU_SECRET_KEY=\"<你的百度securet key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，类似于基于 LangChain 调用 ChatGPT，我们可以实例化一个 LLM 对象并基于此调用文心大模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "PydanticUserError",
     "evalue": "If you use `@root_validator` with pre=False (the default) you MUST specify `skip_on_failure=True`. Note that `@root_validator` is deprecated and should be replaced with `@model_validator`.\n\nFor further information visit https://errors.pydantic.dev/2.3/u/root-validator-pre-skip",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPydanticUserError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/logan/project/Tutorial_for_developing_LLM_application/notebook/C2 调用大模型 API/3. 调用百度文心.ipynb 单元格 31\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blab/home/logan/project/Tutorial_for_developing_LLM_application/notebook/C2%20%E8%B0%83%E7%94%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B%20API/3.%20%E8%B0%83%E7%94%A8%E7%99%BE%E5%BA%A6%E6%96%87%E5%BF%83.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_wenxin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchat_models\u001b[39;00m \u001b[39mimport\u001b[39;00m ChatWenxin\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blab/home/logan/project/Tutorial_for_developing_LLM_application/notebook/C2%20%E8%B0%83%E7%94%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B%20API/3.%20%E8%B0%83%E7%94%A8%E7%99%BE%E5%BA%A6%E6%96%87%E5%BF%83.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mschema\u001b[39;00m \u001b[39mimport\u001b[39;00m HumanMessage\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blab/home/logan/project/Tutorial_for_developing_LLM_application/notebook/C2%20%E8%B0%83%E7%94%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B%20API/3.%20%E8%B0%83%E7%94%A8%E7%99%BE%E5%BA%A6%E6%96%87%E5%BF%83.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m llm \u001b[39m=\u001b[39m ChatWenxin()\n",
      "File \u001b[0;32m~/.conda/envs/zyh_llm/lib/python3.9/site-packages/langchain_wenxin/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# SPDX-FileCopyrightText: 2023-present Tao Yang <swulling@gmail.com>\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# SPDX-License-Identifier: MIT\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_wenxin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchat_models\u001b[39;00m \u001b[39mimport\u001b[39;00m ChatWenxin  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_wenxin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m WenxinClient  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_wenxin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membeddings\u001b[39;00m \u001b[39mimport\u001b[39;00m WenxinEmbeddings  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/zyh_llm/lib/python3.9/site-packages/langchain_wenxin/chat_models.py:23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mschema\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutput\u001b[39;00m \u001b[39mimport\u001b[39;00m ChatGenerationChunk\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpydantic\u001b[39;00m \u001b[39mimport\u001b[39;00m Extra\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_wenxin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllms\u001b[39;00m \u001b[39mimport\u001b[39;00m BaiduCommon\n\u001b[1;32m     26\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mChatWenxin\u001b[39;00m(BaseChatModel, BaiduCommon):\n\u001b[1;32m     27\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Wrapper around Baidu Wenxin's large language model.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[39m    To use, you should have the environment variable ``BAIDU_API_KEY`` and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m            response = model(\"What are the biggest risks facing humanity?\")\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/zyh_llm/lib/python3.9/site-packages/langchain_wenxin/llms.py:20\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_wenxin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m WenxinClient\n\u001b[1;32m     17\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBaiduCommon\u001b[39;00m(BaseModel):\n\u001b[1;32m     21\u001b[0m     client: Any \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m#: :meta private:\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     model: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mernie-bot\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/zyh_llm/lib/python3.9/site-packages/langchain_wenxin/llms.py:52\u001b[0m, in \u001b[0;36mBaiduCommon\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m baidu_secret_key: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Baidu Cloud secret key.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[39m@root_validator\u001b[39m()\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidate_environment\u001b[39m(\u001b[39mcls\u001b[39m, values: Dict) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict:  \u001b[39m# noqa: N805\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate that api key and python package exists in environment.\"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     baidu_api_key \u001b[39m=\u001b[39m get_from_dict_or_env(\n\u001b[1;32m     56\u001b[0m         values, \u001b[39m\"\u001b[39m\u001b[39mbaidu_api_key\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mBAIDU_API_KEY\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/zyh_llm/lib/python3.9/site-packages/pydantic/deprecated/class_validators.py:228\u001b[0m, in \u001b[0;36mroot_validator\u001b[0;34m(pre, skip_on_failure, allow_reuse, *__args)\u001b[0m\n\u001b[1;32m    226\u001b[0m mode: Literal[\u001b[39m'\u001b[39m\u001b[39mbefore\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mafter\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbefore\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m pre \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mafter\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m pre \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m skip_on_failure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 228\u001b[0m     \u001b[39mraise\u001b[39;00m PydanticUserError(\n\u001b[1;32m    229\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIf you use `@root_validator` with pre=False (the default) you MUST specify `skip_on_failure=True`.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    230\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m Note that `@root_validator` is deprecated and should be replaced with `@model_validator`.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    231\u001b[0m         code\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mroot-validator-pre-skip\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    232\u001b[0m     )\n\u001b[1;32m    234\u001b[0m wrap \u001b[39m=\u001b[39m partial(_decorators_v1\u001b[39m.\u001b[39mmake_v1_generic_root_validator, pre\u001b[39m=\u001b[39mpre)\n\u001b[1;32m    236\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdec\u001b[39m(f: Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, Any] \u001b[39m|\u001b[39m \u001b[39mclassmethod\u001b[39m[Any, Any, Any] \u001b[39m|\u001b[39m \u001b[39mstaticmethod\u001b[39m[Any, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n",
      "\u001b[0;31mPydanticUserError\u001b[0m: If you use `@root_validator` with pre=False (the default) you MUST specify `skip_on_failure=True`. Note that `@root_validator` is deprecated and should be replaced with `@model_validator`.\n\nFor further information visit https://errors.pydantic.dev/2.3/u/root-validator-pre-skip"
     ]
    }
   ],
   "source": [
    "from langchain_wenxin.chat_models import ChatWenxin\n",
    "from langchain.schema import HumanMessage\n",
    "llm = ChatWenxin()\n",
    "print(llm([HumanMessage(content=\"你好\")]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zyh_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7cf53b5",
   "metadata": {
    "height": 30
   },
   "source": [
    "# 第六章 Gradio 的介绍与前端界面的搭建 💬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ce5955",
   "metadata": {
    "height": 30
   },
   "source": [
    "Gradio 是一种快速便捷的方法，可以直接在 **Python 中通过友好的 Web 界面演示机器学习模型**。在本课程中，我们将学习*如何使用它为生成式人工智能应用程序构建用户界面*。在构建了应用程序的机器学习或生成式人工智能后，如果你想构建一个demo给其他人看，也许是为了获得反馈并推动系统的改进，或者只是因为你觉得这个系统很酷，所以想演示一下：Gradio 可以让您通过 Python 接口程序快速实现这一目标，而无需编写任何前端、网页或 JavaScript 代码。\n",
    "加载 HF API 密钥和相关 Python 库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a09f0255",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['ZHIPUAI_API_KEY'] ='99ee3e4eb4477848c7a44c0e154a9018.3a7bL4l15TkF3wPi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fa6fa00-6bd1-4839-bcaf-8bae9267ee79",
   "metadata": {
    "height": 199
   },
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import os                # 用于操作系统相关的操作，例如读取环境变量\n",
    "import io                # 用于处理流式数据（例如文件流）\n",
    "import IPython.display   # 用于在IPython环境中显示数据，例如图片\n",
    "import requests          # 用于进行HTTP请求，例如GET和POST请求\n",
    "import zhipuai\n",
    "from zhipuai_llm import ZhipuAILLM\n",
    "\n",
    "# 设置请求的默认超时时间为60秒\n",
    "requests.adapters.DEFAULT_TIMEOUT = 60\n",
    "\n",
    "# 导入dotenv库的函数\n",
    "# dotenv允许您从.env文件中读取环境变量\n",
    "# 这在开发时特别有用，可以避免将敏感信息（如API密钥）硬编码到代码中\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# 寻找.env文件并加载它的内容\n",
    "# 这允许您使用os.environ来读取在.env文件中设置的环境变量\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# 从环境变量中读取'ZHIPUAI_API_KEY'并将其存储在hf_api_key变量中\n",
    "zhipuai.api_key = os.environ['ZHIPUAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f627a",
   "metadata": {
    "height": 30
   },
   "source": [
    "我们在这里设置token和辅助函数。你可以看到，在这里我们使用了不同的库。我们使用的是文本生成库，这是一个用于处理开源 LLM 的精简库，可以让你同时加载 API（就像我们在这里做的一样），也可以在本地运行你自己的 LLM。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "095da8fe-24aa-4dc7-8e08-aa2f949ae21f",
   "metadata": {
    "height": 131
   },
   "outputs": [],
   "source": [
    "# 助手函数\n",
    "llm = ZhipuAILLM(model=\"chatglm_std\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d034a95",
   "metadata": {
    "height": 30
   },
   "source": [
    "## 建立一个应用程序，与任何LLM聊天！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85eeeba",
   "metadata": {
    "height": 30
   },
   "source": [
    "在第 2 课中，我们使用了一个非常简单的 Gradio 界面，它有一个文本框输入和一个输出。在这里，我们也可以用类似的方式与 LLM 聊天。再次复制我们的prompt。在这里，我们可以决定需要多少token。这就是非常简单地向 LLM 提问的方法。但我们还是不能聊天，因为如果你再问一个后续问题，它就无法理解或保留上下文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc9a9159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from typing import (\n",
    "    Any,\n",
    "    AsyncIterator,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Optional,\n",
    ")\n",
    "\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForLLMRun,\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.pydantic_v1 import Field, root_validator\n",
    "from langchain.schema.output import GenerationChunk\n",
    "from langchain.utils import get_from_dict_or_env\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ZhipuAILLM(LLM):\n",
    "    \"\"\"Zhipuai hosted open source or customized models.\n",
    "\n",
    "    To use, you should have the ``zhipuai`` python package installed, and\n",
    "    the environment variable ``qianfan_ak`` and ``qianfan_sk`` set with\n",
    "    your API key and Secret Key.\n",
    "\n",
    "    ak, sk are required parameters which you could get from\n",
    "    https://cloud.baidu.com/product/wenxinworkshop\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            from langchain.llms import QianfanLLMEndpoint\n",
    "            qianfan_model = QianfanLLMEndpoint(model=\"ERNIE-Bot\",\n",
    "                endpoint=\"your_endpoint\", ak=\"your_ak\", sk=\"your_sk\")\n",
    "    \"\"\"\n",
    "\n",
    "    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "    client: Any\n",
    "\n",
    "    model: str = \"chatglm_std\"\n",
    "    \"\"\"Model name in chatglm_pro, chatglm_std, chatglm_lite. \"\"\"\n",
    "\n",
    "    zhipuai_api_key: Optional[str] = None\n",
    "\n",
    "    incremental: Optional[bool] = True\n",
    "    \"\"\"Whether to incremental the results or not.\"\"\"\n",
    "\n",
    "    streaming: Optional[bool] = False\n",
    "    \"\"\"Whether to streaming the results or not.\"\"\"\n",
    "    # streaming = -incremental\n",
    "\n",
    "    request_timeout: Optional[int] = 60\n",
    "    \"\"\"request timeout for chat http requests\"\"\"\n",
    "\n",
    "    top_p: Optional[float] = 0.8\n",
    "    temperature: Optional[float] = 0.95\n",
    "    request_id: Optional[float] = None\n",
    "\n",
    "    @root_validator()\n",
    "    def validate_enviroment(cls, values: Dict) -> Dict:\n",
    "        values[\"zhipuai_api_key\"] = get_from_dict_or_env(\n",
    "            values,\n",
    "            \"zhipuai_api_key\",\n",
    "            \"ZHIPUAI_API_KEY\",\n",
    "        )\n",
    "\n",
    "        params = {\n",
    "            \"api_key\": values[\"zhipuai_api_key\"],\n",
    "            \"model\": values[\"model\"],\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            import zhipuai\n",
    "\n",
    "            values[\"client\"] = zhipuai.model_api\n",
    "        except ImportError:\n",
    "            raise ValueError(\n",
    "                \"zhipuai package not found, please install it with \"\n",
    "                \"`pip install zhipuai`\"\n",
    "            )\n",
    "        return values\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            **{\"model\": self.model},\n",
    "            **super()._identifying_params,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of llm.\"\"\"\n",
    "        return \"zhipuai\"\n",
    "\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get the default parameters for calling OpenAI API.\"\"\"\n",
    "        normal_params = {\n",
    "            \"streaming\" :self.streaming,\n",
    "            \"top_p\": self.top_p,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"request_id\": self.request_id,\n",
    "        }\n",
    "\n",
    "        return {**normal_params, **self.model_kwargs}\n",
    "\n",
    "    def _convert_prompt_msg_params(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        **kwargs: Any,\n",
    "    ) -> dict:\n",
    "        return {\n",
    "            **{\"prompt\": prompt, \"model\": self.model},\n",
    "            **self._default_params,\n",
    "            **kwargs,\n",
    "        }\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Call out to an zhipuai models endpoint for each generation with a prompt.\n",
    "        Args:\n",
    "            prompt: The prompt to pass into the model.\n",
    "        Returns:\n",
    "            The string generated by the model.\n",
    "\n",
    "        Example:\n",
    "            .. code-block:: python\n",
    "                response = zhipuai_model(\"Tell me a joke.\")\n",
    "        \"\"\"\n",
    "        if self.streaming:\n",
    "            completion = \"\"\n",
    "            for chunk in self._stream(prompt, stop, run_manager, **kwargs):\n",
    "                completion += chunk.text\n",
    "            return completion\n",
    "        params = self._convert_prompt_msg_params(prompt, **kwargs)\n",
    "        response_payload = self.client.invoke(**params)\n",
    "\n",
    "        print(response_payload)\n",
    "        return response_payload[\"data\"][\"choices\"][-1][\"content\"]\n",
    "\n",
    "    async def _acall(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        if self.streaming:\n",
    "            completion = \"\"\n",
    "            async for chunk in self._astream(prompt, stop, run_manager, **kwargs):\n",
    "                completion += chunk.text\n",
    "            return completion\n",
    "\n",
    "        params = self._convert_prompt_msg_params(prompt, **kwargs)\n",
    "        response = await self.client.async_invoke(**params)\n",
    "\n",
    "        return response_payload\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[GenerationChunk]:\n",
    "        params = self._convert_prompt_msg_params(prompt, **kwargs)\n",
    "\n",
    "        for res in self.client.invoke(**params):\n",
    "            if res:\n",
    "                chunk = GenerationChunk(text=res)\n",
    "                yield chunk\n",
    "                if run_manager:\n",
    "                    run_manager.on_llm_new_token(chunk.text)\n",
    "\n",
    "    async def _astream(\n",
    "\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> AsyncIterator[GenerationChunk]:\n",
    "        params = self._convert_prompt_msg_params(prompt, **kwargs)\n",
    "\n",
    "        async for res in await self.client.ado(**params):\n",
    "            if res:\n",
    "                chunk = GenerationChunk(text=res[\"data\"][\"choices\"][\"content\"])\n",
    "\n",
    "                yield chunk\n",
    "                if run_manager:\n",
    "                    await run_manager.on_llm_new_token(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a362085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code': 200, 'msg': '操作成功', 'data': {'request_id': '7982261728276681250', 'task_id': '7982261728276681250', 'task_status': 'SUCCESS', 'choices': [{'role': 'assistant', 'content': '\" NIH（National Institutes of Health，美国国立卫生研究院）是美国政府旗下的一个机构，主要负责生物医学研究和公共卫生领域的支持和管理。它成立于 1870 年，总部位于美国马里兰州贝塞斯达市。\\\\n\\\\nNIH 主要由 27 个研究所和中心组成，其中包括美国国家癌症研究所、美国国家心血管病研究所、美国国立糖尿病、消化和肾脏病研究所等。它主要负责资助生物医学研究，促进公共健康和医疗水平的提高。同时，NIH 也负责监管和指导生物医学研究的行为和伦理，确保研究的可靠性和公正性。\\\\n\\\\n在 2023 年，NIH 的预算约为 47 亿美元，是全球最大的生物医学研究资金提供者之一。它的研究成果广泛应用于临床实践和公共健康领域，对全球医疗健康水平的提高做出了重要贡献。\"'}], 'usage': {'prompt_tokens': 137, 'completion_tokens': 177, 'total_tokens': 314}}, 'success': True}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\" NIH（National Institutes of Health，美国国立卫生研究院）是美国政府旗下的一个机构，主要负责生物医学研究和公共卫生领域的支持和管理。它成立于 1870 年，总部位于美国马里兰州贝塞斯达市。\\\\n\\\\nNIH 主要由 27 个研究所和中心组成，其中包括美国国家癌症研究所、美国国家心血管病研究所、美国国立糖尿病、消化和肾脏病研究所等。它主要负责资助生物医学研究，促进公共健康和医疗水平的提高。同时，NIH 也负责监管和指导生物医学研究的行为和伦理，确保研究的可靠性和公正性。\\\\n\\\\n在 2023 年，NIH 的预算约为 47 亿美元，是全球最大的生物医学研究资金提供者之一。它的研究成果广泛应用于临床实践和公共健康领域，对全球医疗健康水平的提高做出了重要贡献。\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ZhipuAILLM(model=\"chatglm_std\")\n",
    "llm.predict(\"nih\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477adf37",
   "metadata": {},
   "source": [
    "### 2.1 参数解析\n",
    "\n",
    "- fn=generate: 这是用于处理输入的函数，即文本生成函数 generate。\n",
    "- inputs=[\n",
    "        gr.Textbox(label=\"Prompt\"),\n",
    "        gr.Slider(label=\"Temperature\", value=0,  maximum=1, minimum=0)。\n",
    "    ]: 这定义了模型的输入。\n",
    "    使用 gr.Textbox 部件来以文本框的形式显示输入的内容描述，label 参数设置了输入部件的标签为 prompt。\n",
    "    使用 gr.Slider 部件以滑动条的形式来显示输入的内容描述，label 参数设置了输入部件的标签为 temperature。\n",
    "- outputs=[gr.Textbox(label=\"Caption\")]: 这定义了输出部分。使用 gr.Textbox 部件来显示生成的内容描述，label 参数设置了输出部件的标签。\n",
    "- title=\"Chat Robot\": 这是界面的标题，将显示在界面的顶部。\n",
    "- description=\"Local Knowledge Base Q&A with llm \": 这是界面的描述，提供有关界面功能的更多信息。\n",
    "- allow_flagging=\"never\": 这设置了不允许标记内容，确保不会显示标记不恰当内容的选项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dcb659e-b71b-46da-b9d2-6ee62498995f",
   "metadata": {
    "height": 182
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入所需的库\n",
    "import gradio as gr  # 用于创建Web界面\n",
    "import os  # 用于与操作系统交互，如读取环境变量\n",
    "\n",
    "# 定义一个函数来根据输入生成文本\n",
    "def generate(input, slider):\n",
    "    # 使用预定义的client对象的generate方法，从输入生成文本\n",
    "    # slider的值限制生成的token的数量\n",
    "    output = llm.predict(input, temperature=slider)\n",
    "    return output  # 返回生成的文本\n",
    "\n",
    "# 创建一个Web界面\n",
    "# 输入：一个文本框和一个滑块\n",
    "# 输出：一个文本框显示生成的文本\n",
    "demo = gr.Interface(\n",
    "    fn=generate, \n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Prompt\"),  # 文本输入框\n",
    "        gr.Slider(label=\"Temperature\", value=0,  maximum=1, minimum=0)  # 滑块用于选择模型的 temperature\n",
    "    ], \n",
    "    outputs=[gr.Textbox(label=\"Completion\")],  # 显示生成文本的文本框\n",
    "    title=\"Chat Robot\",  # 界面标题\n",
    "    description=\"Local Knowledge Base Q&A with llm\",  # 界面描述\n",
    "    # allow_flagging=\"never\", \n",
    ")\n",
    "\n",
    "# 关闭可能已经启动的任何先前的gradio实例\n",
    "gr.close_all()\n",
    "\n",
    "# 启动Web界面\n",
    "# 使用环境变量PORT1作为服务器的端口号\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT1']))\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f80b1",
   "metadata": {},
   "source": [
    "现在我们已经搭建了一个非常简单的 Gradio 界面，它有一个文本框输入和一个输出。我们已经可以非常简单地向 LLM 提问。但我们还是不能聊天，因为如果你再问一个后续问题，它就无法理解或保留上下文。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e5b428",
   "metadata": {
    "height": 30
   },
   "source": [
    "因此，基本上我们要做的是，向模型发送我们之前的问题、它自己的回答以及后续问题。但建立所有这些都有点麻烦。这就是 Gradio 聊天机器人组件的作用所在，因为它允许我们简化向模型发送对话历史记录的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc48b74",
   "metadata": {
    "height": 30
   },
   "source": [
    "因此，我们要解决这个问题。为此，我们将引入一个新的 Gradio 组件--Gradio Chatbot。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1943fd-213a-48bb-966e-e84b9ae255b1",
   "metadata": {},
   "source": [
    "![math](images/ch06_math.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33547d43",
   "metadata": {
    "height": 30
   },
   "source": [
    "## 使用 `gr.Chatbot()` 来助力!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83360647",
   "metadata": {
    "height": 30
   },
   "source": [
    "让我们开始使用 Gradio Chatbot 组件。这里实例化了一个带有文本框提示和提交按钮的 Gradle ChatBot 组件，是一个非常简单的用户界面。但我们现在还不是在和 LLM 聊天。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c70dcae",
   "metadata": {
    "height": 30
   },
   "source": [
    "只需随机选择三个预设回复，然后将我的信息和机器人的信息添加到聊天记录中。所以在这里，你可以看到我可以说任何话，它基本上会随机查看这三个回复。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0179df18-bad5-430d-a91b-1fb0c972d3ca",
   "metadata": {},
   "source": [
    "![math_with_template](images/ch06_math_with_template.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d932fde-da5e-47f1-959b-86b053bb9a42",
   "metadata": {},
   "source": [
    "我们必须格式化聊天prompt。此处正在定义这个格式化聊天prompt函数。\n",
    "在这里，我们要做的就是使其包含聊天历史记录，这样 LLM 就能知道上下文。\n",
    "但这还不够。我们还需要告诉它，哪些信息来自用户，哪些信息来自 LLM 本身，也就是我们正在调用的助手。\n",
    "因此，我们设置了格式聊天prompt功能，在聊天记录的每一轮中，都包含一条用户信息和一条助手信息，以便我们的模型能准确回答后续问题。\n",
    "现在，我们要将格式化的prompt传递给我们的 API。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "452e9b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55bae99d-7a63-4a40-bab7-de7d10b8ab1b",
   "metadata": {
    "height": 471
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义一个函数，用于格式化聊天提示。\n",
    "def format_chat_prompt(message, chat_history):\n",
    "    # 初始化一个空字符串，用于存放格式化后的聊天提示。\n",
    "    prompt = \"\"\n",
    "    # 遍历聊天历史记录。\n",
    "    for turn in chat_history:\n",
    "        # 从聊天记录中提取用户和机器人的消息。\n",
    "        user_message, bot_message = turn\n",
    "        # 更新提示，加入用户和机器人的消息。\n",
    "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    # 将当前的用户消息也加入到提示中，并预留一个位置给机器人的回复。\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    # 返回格式化后的提示。\n",
    "    return prompt\n",
    "\n",
    "# 定义一个函数，用于生成机器人的回复。\n",
    "def respond(message, chat_history):\n",
    "    # 调用上面的函数，将用户的消息和聊天历史记录格式化为一个提示。\n",
    "    formatted_prompt = format_chat_prompt(message, chat_history)\n",
    "    # 使用client对象的generate方法生成机器人的回复（注意：client对象在此代码中并未定义）。\n",
    "    bot_message = client.predict(formatted_prompt,\n",
    "                                  max_new_tokens=1024,\n",
    "                                  stop_sequences=[\"\\nUser:\", \"\"])\n",
    "    # 将用户的消息和机器人的回复加入到聊天历史记录中。\n",
    "    chat_history.append((message, bot_message))\n",
    "    # 返回一个空字符串和更新后的聊天历史记录（这里的空字符串可以替换为真正的机器人回复，如果需要显示在界面上）。\n",
    "    return \"\", chat_history\n",
    "\n",
    "# 下面的代码是设置Gradio界面的部分。\n",
    "\n",
    "# 使用Gradio的Blocks功能定义一个代码块。\n",
    "with gr.Blocks() as demo:\n",
    "    # 创建一个Gradio聊天机器人组件，设置其高度为240。\n",
    "    chatbot = gr.Chatbot(height=240) \n",
    "    # 创建一个文本框组件，用于输入提示。\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    # 创建一个提交按钮。\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    # 创建一个清除按钮，用于清除文本框和聊天机器人组件的内容。\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    # 设置按钮的点击事件。当点击时，调用上面定义的respond函数，并传入用户的消息和聊天历史记录，然后更新文本框和聊天机器人组件。\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    # 设置文本框的提交事件（即按下Enter键时）。功能与上面的按钮点击事件相同。\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) \n",
    "\n",
    "# 关闭所有已经存在的Gradio实例。\n",
    "gr.close_all()\n",
    "# 启动新的Gradio应用，设置分享功能为True，并使用环境变量PORT3指定服务器端口。\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT3']))\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e45875a-8ca4-4124-982e-f2ee6c6e597a",
   "metadata": {},
   "source": [
    "![animal](images/ch06_animal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022bb649-868d-453d-95e4-fef9cb1feadd",
   "metadata": {},
   "source": [
    "![animal_in_context](images/ch06_animal_in_context.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae2ad9",
   "metadata": {
    "height": 30
   },
   "source": [
    "现在，我们的聊天机器人应该可以回答后续问题了。\n",
    "我们可以看到，我们向它发送了上下文。我们向它发送了信息，然后要求它完成。一旦我们进入另一个迭代循环，我们就会向它发送我们的整个上下文，然后要求它完成。这很酷。但是，如果我们一直这样迭代下去，那么模型在一次对话中所能接受的信息量就会达到极限，因为我们总是给它越来越多的之前对话的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250fcfff",
   "metadata": {
    "height": 46
   },
   "source": [
    "为了让模型发挥最大作用，我们可以在这里将最大token数`max_new_tokens`设置为 1024、 这是我们在 API 中运行的硬件条件下，该模型所能接受的最大值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b9d8d",
   "metadata": {
    "height": 64
   },
   "source": [
    "可以尝试以下prompt：\n",
    "1. 哪些动物生活在热带草原？\n",
    "2. 这之中哪种动物最强壮？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ded928f",
   "metadata": {
    "height": 30
   },
   "source": [
    "这里，我们创建了一个简单但功能强大的用户界面，用于与LLM聊天。如果需要进一步Gradio 所能提供的最佳功能，我们可以创建一个包含更多功能的用户界面。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b69830",
   "metadata": {
    "height": 30
   },
   "source": [
    "### 添加其他高级功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09873dfd-5b6c-41d6-9479-12e8c8894295",
   "metadata": {
    "height": 828
   },
   "outputs": [],
   "source": [
    "# 定义一个函数，用于格式化聊天提示。\n",
    "def format_chat_prompt(message, chat_history, instruction):\n",
    "    # 初始化提示，加入系统指令。\n",
    "    prompt = f\"System:{instruction}\"\n",
    "    # 遍历聊天历史记录。\n",
    "    for turn in chat_history:\n",
    "        # 从聊天记录中提取用户和机器人的消息。\n",
    "        user_message, bot_message = turn\n",
    "        # 更新提示，加入用户和机器人的消息。\n",
    "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    # 将当前的用户消息也加入到提示中，并预留一个位置给机器人的回复。\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    # 返回格式化后的提示。\n",
    "    return prompt\n",
    "\n",
    "# 定义一个函数，用于生成机器人的回复。\n",
    "def respond(message, chat_history, instruction, temperature=0.7):\n",
    "    # 调用上面的函数，将用户的消息、聊天历史记录和系统指令格式化为一个提示。\n",
    "    prompt = format_chat_prompt(message, chat_history, instruction)\n",
    "    # 更新聊天历史记录，先加入用户的消息（机器人的回复部分先为空）。\n",
    "    chat_history = chat_history + [[message, \"\"]]\n",
    "    # 使用client对象的generate_stream方法生成机器人的回复（注意：client对象在此代码中并未定义）。\n",
    "    stream = client.generate_stream(prompt,\n",
    "                                    max_new_tokens=1024,\n",
    "                                    stop_sequences=[\"\\nUser:\", \"\"], \n",
    "                                    temperature=temperature)  # 设置生成回复的温度，决定回复的随机性。\n",
    "    acc_text = \"\"\n",
    "    # 使用流式处理获取机器人的回复。\n",
    "    for idx, response in enumerate(stream):\n",
    "        text_token = response.token.text\n",
    "\n",
    "        # 如果有任何详情信息，直接返回。\n",
    "        if response.details:\n",
    "            return\n",
    "\n",
    "        # 如果是第一个令牌并且它以空格开始，则去除该空格。\n",
    "        if idx == 0 and text_token.startswith(\" \"):\n",
    "            text_token = text_token[1:]\n",
    "\n",
    "        # 累积生成的文本。\n",
    "        acc_text += text_token\n",
    "        # 更新最后一轮的聊天记录。\n",
    "        last_turn = list(chat_history.pop(-1))\n",
    "        last_turn[-1] += acc_text\n",
    "        chat_history = chat_history + [last_turn]\n",
    "        yield \"\", chat_history\n",
    "        acc_text = \"\"\n",
    "\n",
    "# 设置Gradio界面部分。\n",
    "with gr.Blocks() as demo:\n",
    "    # 创建一个Gradio聊天机器人组件，并设置其高度。\n",
    "    chatbot = gr.Chatbot(height=240)\n",
    "    # 创建一个文本框组件，用于输入提示。\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    # 创建一个可折叠组件，用于显示高级选项。\n",
    "    with gr.Accordion(label=\"Advanced options\", open=False):\n",
    "        # 在可折叠组件内创建一个文本框，用于输入系统消息。\n",
    "        system = gr.Textbox(label=\"System message\", lines=2, value=\"一段用户和基于大语言模型的法律助手的对话. 助手会给出真实且有帮助的回答.\")\n",
    "        # 创建一个滑块，用于调整回复的温度。\n",
    "        temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1, value=0.7, step=0.1)\n",
    "    # 创建一个提交按钮。\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    # 创建一个清除按钮，用于清除文本框和聊天机器人组件的内容。\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    # 设置按钮的点击事件。当点击时，调用上面定义的respond函数，并传入用户的消息、聊天历史记录和系统消息，然后更新文本框和聊天机器人组件。\n",
    "    btn.click(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot])\n",
    "    # 设置文本框的提交事件（即按下Enter键时）。功能与上面的按钮点击事件相同。\n",
    "    msg.submit(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf44cb6-c55c-45eb-8aca-c60f08aa1e0f",
   "metadata": {},
   "source": [
    "![law_1](images/ch06_law_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70256e38-a97f-4375-9083-720571c6cc2c",
   "metadata": {},
   "source": [
    "![law_2](images/ch06_law_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e821c9-f568-4cf5-92b9-dee9f19a45d8",
   "metadata": {},
   "source": [
    "![law_3](images/ch06_law_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a928b-1620-4bb0-ab80-0c78ad47b96d",
   "metadata": {},
   "source": [
    "在这里，我们有高级选项，包括系统消息，它可以设置 LLM 与你聊天的模式。\n",
    "因此，在系统消息中，你可以说，例如，你是一个乐于助人的助手，或者你可以给它一个特定的语气，一个特定的语调，\n",
    "你希望它更有趣一点，更严肃一点，你真的可以反复调试系统消息提示，看看它对你的消息有什么影响。\n",
    "\n",
    "有些人甚至会想给 LLM 一个角色，比如你是一个提供法律建议的律师，或者你是一个提供医疗建议的医生，\n",
    "但要注意的是，众所周知，LLM 会以一种听起来很真实的方式提供虚假信息。\n",
    "因此，尽管使用Falcon 40B 进行实验和探索会很有趣，但在现实世界的应用场景中，必须为此类使用案例制定进一步的保障措施。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e0e822-ec15-4cfe-9bfe-e90250fbd85e",
   "metadata": {},
   "source": [
    "还有其他高级参数，比如这里的温度。\n",
    "温度基本上就是你希望模型的变化程度。因此，如果将温度设为零，模型就会倾向于始终对相同的输入做出相同的反应。\n",
    "所以同样的问题，同样的答案。温度越高，信息的变化就越多。但如果温度过高，它就会开始给出无意义的答案。\n",
    "因此，0.7 是一个不错的默认参数，但我们鼓励你多做尝试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183d7964",
   "metadata": {
    "height": 30
   },
   "source": [
    "除此之外，这个用户界面还能让我们进行流式传输回复。\n",
    "它逐个token发送，我们可以看到它实时完成。因此，我们不需要等到整个答案都准备好了。在这里，我们可以看到它是如何完成的。如果你不理解这里的所有内容，也不用担心，因为我们的目的是用一个非常完整的用户界面来结束课程，并提供LLM方面的所有功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979cc4b6",
   "metadata": {
    "height": 30
   },
   "source": [
    "在格式聊天提示中，也就是我们之前使用的功能中，我们添加了一个新元素，那就是系统指令。因此，在开始用户助手对话之前，我们在系统顶部添加了一个指令。因此，基本上在发送给模型的每条信息的开头，都会有我们设置的系统信息。在这里，我们调用文本生成库的`generate_stream`函数。而 `generate_stream`函数的作用就是逐个生成响应标记。因此，在这个循环中，发生的事情就是按标记生成响应标记，将其添加到聊天记录中，然后将其返回给函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d9ec80a-39ad-4f58-b79e-4f413c5074c0",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "# 关闭可能已经启动的任何先前的gradio实例\n",
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cf21ba",
   "metadata": {},
   "source": [
    "现在我们可以将本地数据库的内容接入进来，让 llm 通过本地数据库进行回答。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21aa5b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from zhipuai_embedding import ZhipuAIEmbeddings\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from zhipuai_llm import ZhipuAILLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d30d71aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98eec0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = '../docs/chroma/knowledge_base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a338fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入检索式问答链\n",
    "from langchain.chains import RetrievalQA\n",
    "def chat_with_db(query, chat_history):\n",
    "\n",
    "    llm = ZhipuAILLM(model=\"chatglm_std\")\n",
    "\n",
    "    embedding = ZhipuAIEmbeddings()\n",
    "    vectordb = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embedding\n",
    "    )\n",
    "    # Build prompt\n",
    "    template = \"\"\"使用以下上下文片段来回答最后的问题。如果你不知道答案，只需说不知道，不要试图编造答案。答案最多使用三个句子。尽量简明扼要地回答。在回答的最后一定要说\"感谢您的提问！\"\n",
    "    {context}\n",
    "    问题：{question}\n",
    "    有用的回答：\"\"\"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "    # qa_chain = ConversationalRetrievalChain.from_llm(llm, vectordb.as_retriever())\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=vectordb.as_retriever(),\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    "    )\n",
    "    result = qa_chain({\"query\": query, \"context\": chat_history})\n",
    "    \n",
    "    # 将用户的消息和机器人的回复加入到聊天历史记录中。\n",
    "    chat_history.append((query, result['result']))\n",
    "    # 返回一个空字符串和更新后的聊天历史记录（这里的空字符串可以替换为真正的机器人回复，如果需要显示在界面上）。\n",
    "    return \"\", chat_history\n",
    "\n",
    "    # return result['answer'], chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b8e7c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhihu123/Project/other/Tutorial_for_developing_LLM_application/notebook/C7  前后端搭建\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88ccc512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量库中存储的数量：0\n",
      "{'query': '课程', 'context': []}\n",
      "['query']\n",
      "{'input_documents': [], 'question': '课程'}\n",
      "['input_documents', 'question']\n",
      "{'question': '课程', 'context': ''}\n",
      "['context', 'question']\n"
     ]
    }
   ],
   "source": [
    "# 使用Gradio的Blocks功能定义一个代码块。\n",
    "with gr.Blocks() as demo:\n",
    "    # 创建一个Gradio聊天机器人组件，设置其高度为240。\n",
    "    chatbot = gr.Chatbot(height=240) \n",
    "    # 创建一个文本框组件，用于输入提示。\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    # 创建一个提交按钮。\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    # 创建一个清除按钮，用于清除文本框和聊天机器人组件的内容。\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    # 设置按钮的点击事件。当点击时，调用上面定义的 chat_with_db 函数，并传入用户的消息和聊天历史记录，然后更新文本框和聊天机器人组件。\n",
    "    btn.click(chat_with_db, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    # 设置文本框的提交事件（即按下Enter键时）。功能与上面的按钮点击事件相同。\n",
    "    msg.submit(chat_with_db, inputs=[msg, chatbot], outputs=[msg, chatbot]) \n",
    "\n",
    "# 关闭所有已经存在的Gradio实例。\n",
    "gr.close_all()\n",
    "# 启动新的Gradio应用，设置分享功能为True，并使用环境变量PORT3指定服务器端口。\n",
    "# demo.launch(share=True, server_port=int(os.environ['PORT3']))\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f070c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a19761b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"cells":[{"cell_type":"markdown","id":"b7cf53b5","metadata":{"height":30},"source":["# 第七章 Gradio 的介绍与前端界面的搭建 💬\n","\n"," - [一、Gradio 简介](#一、Gradio-简介)\n","     - [1.1 简单示例](#1.1-简单示例)\n"," - [二、使用 `gr.Chatbot()` 来助力聊天!](#二、使用-`gr.Chatbot()`-来助力聊天!)\n"," - [三、 llm 通过本地数据库进行回答](#三、-llm-通过本地数据库进行回答)\n","     - [3.1 封装函数](#3.1-封装函数)\n","     - [3.2 组装 gradio 界面](#3.2-组装-gradio-界面)\n"]},{"cell_type":"markdown","id":"f8ce5955","metadata":{"height":30},"source":["Gradio 是一种快速便捷的方法，可以直接在 **Python 中通过友好的 Web 界面演示机器学习模型**。在本课程中，我们将学习*如何使用它为生成式人工智能应用程序构建用户界面*。在构建了应用程序的机器学习或生成式人工智能后，如果你想构建一个demo给其他人看，也许是为了获得反馈并推动系统的改进，或者只是因为你觉得这个系统很酷，所以想演示一下：Gradio 可以让您通过 Python 接口程序快速实现这一目标，而无需编写任何前端、网页或 JavaScript 代码。\n","加载 HF API 密钥和相关 Python 库"]},{"cell_type":"markdown","id":"ab0b7d84","metadata":{},"source":["首先我们导入使用到的库"]},{"cell_type":"code","execution_count":2,"id":"0fa6fa00-6bd1-4839-bcaf-8bae9267ee79","metadata":{"height":199},"outputs":[],"source":["# 导入必要的库\n","import os                # 用于操作系统相关的操作，例如读取环境变量\n","import io                # 用于处理流式数据（例如文件流）\n","import IPython.display   # 用于在IPython环境中显示数据，例如图片\n","import requests          # 用于进行HTTP请求，例如GET和POST请求\n","import zhipuai\n","from zhipuai_llm import ZhipuAILLM\n","\n","# 设置请求的默认超时时间为60秒\n","requests.adapters.DEFAULT_TIMEOUT = 60\n","\n","# 导入dotenv库的函数\n","# dotenv允许您从.env文件中读取环境变量\n","# 这在开发时特别有用，可以避免将敏感信息（如API密钥）硬编码到代码中\n","from dotenv import load_dotenv, find_dotenv\n","\n","# 寻找.env文件并加载它的内容\n","# 这允许您使用os.environ来读取在.env文件中设置的环境变量\n","_ = load_dotenv(find_dotenv())\n","\n","# 从环境变量中读取'ZHIPUAI_API_KEY'并将其存储在hf_api_key变量中\n","zhipuai.api_key = os.environ['ZHIPUAI_API_KEY']"]},{"cell_type":"code","execution_count":3,"id":"095da8fe-24aa-4dc7-8e08-aa2f949ae21f","metadata":{"height":131},"outputs":[],"source":["# 助手函数\n","llm = ZhipuAILLM(model=\"chatglm_std\", temperature=0)"]},{"cell_type":"markdown","id":"7d034a95","metadata":{"height":30},"source":["## 一、Gradio 简介"]},{"cell_type":"markdown","id":"ab87273b","metadata":{},"source":["Gradio 可以包装几乎任何 Python 函数为易于使用的用户界面。\n","\n","常用的基础模块构成如下：\n","\n","- 应用界面：gr.Interface(简易场景), gr.Blocks(定制化场景)\n","\n","- 输入输出：gr.Image(图像), gr.Textbox(文本框), gr.DataFrame(数据框), gr.Dropdown(下拉选项), gr.Number(数字), gr.Markdown(Markdown), gr.Files(文件)\n","\n","- 控制组件：gr.Button(按钮)\n","\n","- 布局组件：gr.Tab(标签页), gr.Row(行布局), gr.Column(列布局)"]},{"cell_type":"markdown","id":"258a9a73","metadata":{},"source":["大部分功能模块都可以通过以下三个参数进行初始化：\n","\n","- fn：包装的函数\n","- inputs：输入组件类型，（例如：“text”、\"image）\n","- ouputs：输出组件类型，（例如：“text”、\"image）"]},{"cell_type":"markdown","id":"477adf37","metadata":{},"source":["### 1.1 简单示例\n","\n","- fn=generate: 这是用于处理输入的函数，即文本生成函数 generate。\n","- inputs=[\n","        gr.Textbox(label=\"Prompt\"),\n","        gr.Slider(label=\"Temperature\", value=0,  maximum=1, minimum=0)。\n","    ]: 这定义了模型的输入。\n","    使用 gr.Textbox 部件来以文本框的形式显示输入的内容描述，label 参数设置了输入部件的标签为 prompt。\n","    使用 gr.Slider 部件以滑动条的形式来显示输入的内容描述，label 参数设置了输入部件的标签为 temperature。\n","- outputs=[gr.Textbox(label=\"Caption\")]: 这定义了输出部分。使用 gr.Textbox 部件来显示生成的内容描述，label 参数设置了输出部件的标签。\n","- title=\"Chat Robot\": 这是界面的标题，将显示在界面的顶部。\n","- description=\"Local Knowledge Base Q&A with llm \": 这是界面的描述，提供有关界面功能的更多信息。\n","- allow_flagging=\"never\": 这设置了不允许标记内容，确保不会显示标记不恰当内容的选项。"]},{"cell_type":"code","execution_count":4,"id":"0dcb659e-b71b-46da-b9d2-6ee62498995f","metadata":{"height":182},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on local URL:  http://127.0.0.1:7860\n","\n","To create a public link, set `share=True` in `launch()`.\n"]},{"data":{"text/html":["<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":[]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# 导入所需的库\n","import gradio as gr  # 用于创建Web界面\n","import os  # 用于与操作系统交互，如读取环境变量\n","\n","# 定义一个函数来根据输入生成文本\n","def generate(input, slider):\n","    # 使用预定义的client对象的generate方法，从输入生成文本\n","    # slider的值限制生成的token的数量\n","    output = llm.predict(input, temperature=slider)\n","    return output  # 返回生成的文本\n","\n","# 创建一个Web界面\n","# 输入：一个文本框和一个滑块\n","# 输出：一个文本框显示生成的文本\n","demo = gr.Interface(\n","    fn=generate, \n","    inputs=[\n","        gr.Textbox(label=\"Prompt\"),  # 文本输入框\n","        gr.Slider(label=\"Temperature\", value=0,  maximum=1, minimum=0)  # 滑块用于选择模型的 temperature\n","    ], \n","    outputs=[gr.Textbox(label=\"Completion\")],  # 显示生成文本的文本框\n","    title=\"Chat Robot\",  # 界面标题\n","    description=\"Local Knowledge Base Q&A with llm\",  # 界面描述\n","    # allow_flagging=\"never\", \n",")\n","\n","# 关闭可能已经启动的任何先前的gradio实例\n","gr.close_all()\n","\n","# 启动Web界面\n","# 使用环境变量PORT1作为服务器的端口号\n","# demo.launch(share=True, server_port=int(os.environ['PORT1']))\n","demo.launch()"]},{"cell_type":"markdown","id":"af5f80b1","metadata":{},"source":["现在我们已经搭建了一个非常简单的 Gradio 界面，它有一个文本框输入和一个输出。我们已经可以非常简单地向 LLM 提问。但我们还是不能对话，因为如果你再问一个后续问题，它就无法理解或保留上下文。"]},{"cell_type":"markdown","id":"79e5b428","metadata":{"height":30},"source":["因此，基本上我们要做的是，向模型发送我们之前的问题、它自己的回答以及后续问题。但建立所有这些都有点麻烦。这就是 Gradio 聊天机器人组件的作用所在，因为它允许我们简化向模型发送对话历史记录的过程。"]},{"cell_type":"markdown","id":"bdc48b74","metadata":{"height":30},"source":["因此，我们要解决这个问题。为此，我们将引入一个新的 Gradio 组件--Gradio Chatbot。"]},{"cell_type":"markdown","id":"33547d43","metadata":{"height":30},"source":["## 二、使用 `gr.Chatbot()` 来助力聊天!"]},{"cell_type":"markdown","id":"83360647","metadata":{"height":30},"source":["让我们开始使用 Gradio Chatbot 组件。这里实例化了一个带有文本框提示和提交按钮的 Gradle ChatBot 组件，是一个非常简单的用户界面。但我们现在还不是在和 LLM 聊天。"]},{"cell_type":"markdown","id":"3d932fde-da5e-47f1-959b-86b053bb9a42","metadata":{},"source":["我们必须格式化聊天prompt。此处正在定义这个格式化聊天prompt函数。\n","在这里，我们要做的就是使其包含聊天历史记录，这样 LLM 就能知道上下文。\n","但这还不够。我们还需要告诉它，哪些信息来自用户，哪些信息来自 LLM 本身，也就是我们正在调用的助手。\n","因此，我们设置了格式聊天prompt功能，在聊天记录的每一轮中，都包含一条用户信息和一条助手信息，以便我们的模型能准确回答后续问题。\n","现在，我们要将格式化的prompt传递给我们的 API。"]},{"cell_type":"markdown","id":"c4a10e7d","metadata":{},"source":["相比 Interface，Blocks 提供了一个低级别的 API，用于设计具有更灵活布局和数据流的网络应用。Blocks 允许控制组件在页面上出现的位置，处理复杂的数据流（例如，输出可以作为其他函数的输入），并根据用户交互更新组件的属性可见性。可以定制更多组件。"]},{"cell_type":"code","execution_count":5,"id":"55bae99d-7a63-4a40-bab7-de7d10b8ab1b","metadata":{"height":471},"outputs":[{"name":"stdout","output_type":"stream","text":["Closing server running on port: 7860\n","Running on local URL:  http://127.0.0.1:7860\n","\n","To create a public link, set `share=True` in `launch()`.\n"]},{"data":{"text/html":["<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":[]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# 定义一个函数，用于格式化聊天提示。\n","def format_chat_prompt(message, chat_history):\n","    # 初始化一个空字符串，用于存放格式化后的聊天提示。\n","    prompt = \"\"\n","    # 遍历聊天历史记录。\n","    for turn in chat_history:\n","        # 从聊天记录中提取用户和机器人的消息。\n","        user_message, bot_message = turn\n","        # 更新提示，加入用户和机器人的消息。\n","        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n","    # 将当前的用户消息也加入到提示中，并预留一个位置给机器人的回复。\n","    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n","    # 返回格式化后的提示。\n","    return prompt\n","\n","# 定义一个函数，用于生成机器人的回复。\n","def respond(message, chat_history):\n","    # 调用上面的函数，将用户的消息和聊天历史记录格式化为一个提示。\n","    formatted_prompt = format_chat_prompt(message, chat_history)\n","    # 使用llm对象的predict方法生成机器人的回复（注意：llm对象在此代码中并未定义）。\n","    bot_message = llm.predict(formatted_prompt,\n","                                  max_new_tokens=1024,\n","                                  stop_sequences=[\"\\nUser:\", \"\"])\n","    # 将用户的消息和机器人的回复加入到聊天历史记录中。\n","    chat_history.append((message, bot_message))\n","    # 返回一个空字符串和更新后的聊天历史记录（这里的空字符串可以替换为真正的机器人回复，如果需要显示在界面上）。\n","    return \"\", chat_history\n","\n","# 下面的代码是设置Gradio界面的部分。\n","\n","# 使用Gradio的Blocks功能定义一个代码块。\n","with gr.Blocks() as demo:\n","    # 创建一个Gradio聊天机器人组件，设置其高度为240。\n","    chatbot = gr.Chatbot(height=240) \n","    # 创建一个文本框组件，用于输入提示。\n","    msg = gr.Textbox(label=\"Prompt\")\n","    # 创建一个提交按钮。\n","    btn = gr.Button(\"Submit\")\n","    # 创建一个清除按钮，用于清除文本框和聊天机器人组件的内容。\n","    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n","\n","    # 设置按钮的点击事件。当点击时，调用上面定义的respond函数，并传入用户的消息和聊天历史记录，然后更新文本框和聊天机器人组件。\n","    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n","    # 设置文本框的提交事件（即按下Enter键时）。功能与上面的按钮点击事件相同。\n","    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) \n","\n","# 关闭所有已经存在的Gradio实例。\n","gr.close_all()\n","# 启动新的Gradio应用，设置分享功能为True，并使用环境变量PORT3指定服务器端口。\n","# demo.launch(share=True, server_port=int(os.environ['PORT3']))\n","demo.launch()"]},{"cell_type":"markdown","id":"4dae2ad9","metadata":{"height":30},"source":["现在，我们的问答机器人可以回答后续问题了。\n","我们可以看到，我们向它发送了上下文。我们向它发送了信息，然后要求它完成。一旦我们进入另一个迭代循环，我们就会向它发送我们的整个上下文，然后要求它完成。这很酷。但是，如果我们一直这样迭代下去，那么模型在一次对话中所能接受的信息量就会达到极限，因为我们总是给它越来越多的之前对话的内容。"]},{"cell_type":"markdown","id":"2ded928f","metadata":{"height":30},"source":["这里，我们创建了一个简单但功能强大的用户界面，用于与LLM聊天。如果需要进一步Gradio 所能提供的最佳功能，我们可以创建一个包含更多功能的用户界面。"]},{"cell_type":"markdown","id":"23b69830","metadata":{"height":30},"source":["## 三、 llm 通过本地数据库进行回答"]},{"cell_type":"markdown","id":"96cf21ba","metadata":{},"source":["现在我们可以将本地数据库的内容接入进来，让 llm 通过本地数据库进行回答。"]},{"cell_type":"code","execution_count":7,"id":"21aa5b11","metadata":{},"outputs":[],"source":["from langchain.vectorstores import Chroma\n","from langchain.document_loaders import PyMuPDFLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n","from zhipuai_embedding import ZhipuAIEmbeddings\n","\n","from langchain.llms import OpenAI\n","from langchain.llms import HuggingFacePipeline\n","from zhipuai_llm import ZhipuAILLM"]},{"cell_type":"code","execution_count":8,"id":"d30d71aa","metadata":{},"outputs":[],"source":["from langchain.prompts import PromptTemplate"]},{"cell_type":"code","execution_count":38,"id":"98eec0f3","metadata":{},"outputs":[],"source":["persist_directory = 'docs/chroma/knowledge_base'"]},{"cell_type":"code","execution_count":13,"id":"a338fd65","metadata":{},"outputs":[],"source":["# 导入检索式问答链\n","from langchain.chains import RetrievalQA\n","def chat_with_db(query, chat_history):\n","\n","    llm = ZhipuAILLM(model=\"chatglm_std\")\n","\n","    embedding = ZhipuAIEmbeddings()\n","    vectordb = Chroma(\n","        persist_directory=persist_directory,\n","        embedding_function=embedding\n","    )\n","    # Build prompt\n","    template = \"\"\"使用以下上下文片段来回答最后的问题。如果你不知道答案，只需说不知道，不要试图编造答案。答案最多使用三个句子。尽量简明扼要地回答。在回答的最后一定要说\"感谢您的提问！\"\n","    {context}\n","    问题：{question}\n","    有用的回答：\"\"\"\n","    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n","    # qa_chain = ConversationalRetrievalChain.from_llm(llm, vectordb.as_retriever())\n","    qa_chain = RetrievalQA.from_chain_type(\n","        llm,\n","        retriever=vectordb.as_retriever(),\n","        return_source_documents=True,\n","        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n","    )\n","    result = qa_chain({\"query\": query, \"context\": chat_history})\n","    \n","    # 将用户的消息和机器人的回复加入到聊天历史记录中。\n","    chat_history.append((query, result['result']))\n","    # 返回一个空字符串和更新后的聊天历史记录（这里的空字符串可以替换为真正的机器人回复，如果需要显示在界面上）。\n","    return \"\", chat_history"]},{"cell_type":"code","execution_count":15,"id":"88ccc512","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Closing server running on port: 7860\n","Running on local URL:  http://127.0.0.1:7862\n","\n","To create a public link, set `share=True` in `launch()`.\n"]},{"data":{"text/html":["<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":[]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# 使用Gradio的Blocks功能定义一个代码块。\n","with gr.Blocks() as demo:\n","    # 创建一个Gradio聊天机器人组件，设置其高度为240。\n","    chatbot = gr.Chatbot(height=240) \n","    # 创建一个文本框组件，用于输入提示。\n","    msg = gr.Textbox(label=\"Prompt\")\n","    # 创建一个提交按钮。\n","    btn = gr.Button(\"Submit\")\n","    # 创建一个清除按钮，用于清除文本框和聊天机器人组件的内容。\n","    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n","\n","    # 设置按钮的点击事件。当点击时，调用上面定义的 chat_with_db 函数，并传入用户的消息和聊天历史记录，然后更新文本框和聊天机器人组件。\n","    btn.click(chat_with_db, inputs=[msg, chatbot], outputs=[msg, chatbot])\n","    # 设置文本框的提交事件（即按下Enter键时）。功能与上面的按钮点击事件相同。\n","    msg.submit(chat_with_db, inputs=[msg, chatbot], outputs=[msg, chatbot]) \n","\n","# 关闭所有已经存在的Gradio实例。\n","gr.close_all()\n","# 启动新的Gradio应用，设置分享功能为True，并使用环境变量PORT3指定服务器端口。\n","# demo.launch(share=True, server_port=int(os.environ['PORT3']))\n","demo.launch()"]},{"cell_type":"markdown","id":"f2de6a4c","metadata":{},"source":["### 3.1 封装函数"]},{"cell_type":"markdown","id":"307af98b","metadata":{},"source":["现在我们可以将所需的功能函数进行封装，将其与 gradio 进行绑定。"]},{"cell_type":"markdown","id":"705c3e2f","metadata":{},"source":["1. 上传文档\n","2. 初始化向量数据库\n","3. 向 LLM 提问\n","4. 清空对话"]},{"cell_type":"code","execution_count":69,"id":"c76a0d75","metadata":{},"outputs":[],"source":["from langchain.document_loaders import PyMuPDFLoader"]},{"cell_type":"code","execution_count":153,"id":"c56f070c","metadata":{},"outputs":[],"source":["def init_knowledge_db(file):\n","    \"\"\"\n","    该函数用于加载 PDF 文件，切分文档，生成文档的嵌入向量，创建向量数据库。\n","\n","    参数:\n","    file (str): 要加载的 PDF 文件路径。\n","\n","    返回:\n","    vectordb (vectordb): 创建的数据库。\n","    \"\"\"\n","    # 载入文档\n","    loader = PyMuPDFLoader(file.name)\n","    documents = loader.load()\n","    # 切分文档\n","    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=150)\n","    split_docs = text_splitter.split_documents(documents)\n","    # 定义 Embeddings\n","    embeddings = ZhipuAIEmbeddings()\n","    # 根据数据创建向量数据库\n","    vectordb = Chroma.from_documents(\n","        documents=split_docs[:3], # 为了速度，只选择了前 100 个切分的 doc 进行生成。\n","        embedding=embedding,\n","        persist_directory=persist_directory  # 允许我们将persist_directory目录保存到磁盘上\n","    )\n","    return vectordb \n"]},{"cell_type":"code","execution_count":154,"id":"c5ffdb7c","metadata":{},"outputs":[],"source":["def load_knowledge_db(path, embeddings):\n","    vectordb = Chroma.from_documents(\n","        embeddings=embeddings,\n","        persist_directory=path.name  \n","    )\n","    vectordb.persist()"]},{"cell_type":"code","execution_count":155,"id":"fbdd8a00","metadata":{},"outputs":[],"source":["def presit_knowledge_db(vectordb):\n","    vectordb.persist()"]},{"cell_type":"code","execution_count":180,"id":"74d5ce93","metadata":{},"outputs":[],"source":["def chat_with_db(query, chat_history, history_len, top_k, temperature):\n","\n","    llm = ZhipuAILLM(model=\"chatglm_std\", temperature=temperature)\n","    embedding = ZhipuAIEmbeddings()\n","    vectordb = Chroma(\n","        persist_directory=persist_directory,\n","        embedding_function=embedding\n","    )\n","    # Build prompt\n","    template = \"\"\"使用以下上下文片段来回答最后的问题。如果你不知道答案，只需说不知道，不要试图编造答案。答案最多使用三个句子。尽量简明扼要地回答。在回答的最后一定要说\"感谢您的提问！\"\n","    {context}\n","    问题：{question}\n","    有用的回答：\"\"\"\n","    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n","    # qa_chain = ConversationalRetrievalChain.from_llm(llm, vectordb.as_retriever())\n","    qa_chain = RetrievalQA.from_chain_type(\n","        llm,\n","        retriever=vectordb.as_retriever(\n","                search_kwargs={\"k\": top_k}),\n","        return_source_documents=True,\n","        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n","    )\n","    # 限制 history 的记忆长度\n","    chat_history = chat_history[-history_len:] if history_len > 0 else []\n","\n","    result = qa_chain({\"query\": query, \"context\": chat_history})\n","    \n","    # 将用户的消息和机器人的回复加入到聊天历史记录中。\n","    chat_history.append((query, result['result']))\n","    # 返回一个空字符串和更新后的聊天历史记录（这里的空字符串可以替换为真正的机器人回复，如果需要显示在界面上）。\n","    return \"\", chat_history"]},{"cell_type":"markdown","id":"3b1a50ca","metadata":{},"source":["### 3.2 组装 gradio 界面"]},{"cell_type":"markdown","id":"58c52c55","metadata":{},"source":["我们可以在 gradio.Markdown 中增加一些描述性写法。"]},{"cell_type":"markdown","id":"d3ffe2c0","metadata":{},"source":["我们可以配置些高级参数，\n","- 向量检索的数量（top_k）：从向量数据库返回的最相关文档的数量，LLM 会根据返回的文档生成回答。\n","- 聊天历史的长度（history_len）：使聊天历史保持一定的长度，防止过大消耗过多的 token。\n","- 温度（temperature）：温度基本上就是你希望模型的变化程度。因此，如果将温度设为零，模型就会倾向于始终对相同的输入做出相同的反应。所以同样的问题，同样的答案。温度越高，信息的变化就越多。但如果温度过高，它就会开始给出无意义的答案。"]},{"cell_type":"code","execution_count":186,"id":"5a19761b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Closing server running on port: 7860\n","Running on local URL:  http://127.0.0.1:7911\n","\n","To create a public link, set `share=True` in `launch()`.\n"]},{"data":{"text/html":["<div><iframe src=\"http://127.0.0.1:7911/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":[]},"execution_count":186,"metadata":{},"output_type":"execute_result"}],"source":["block = gr.Blocks()\n","with block as demo:\n","    gr.Markdown(\"\"\"<h1><center>Chat Robot</center></h1>\n","    <center>Local Knowledge Base Q&A with llm</center>\n","    \"\"\")\n","    with gr.Row():\n","        with gr.Column(scale=4):\n","            chatbot = gr.Chatbot(height=240) \n","            # 创建一个文本框组件，用于输入提示。\n","            msg = gr.Textbox(label=\"Prompt/问题\")\n","\n","            # 设置按钮的点击事件。当点击时，调用上面定义的 chat_with_db 函数，并传入用户的消息和聊天历史记录，然后更新文本框和聊天机器人组件。\n","            btn.click(chat_with_db, inputs=[msg, chatbot, history_len, top_k, temperature], outputs=[msg, chatbot])\n","            # 设置文本框的提交事件（即按下Enter键时）。功能与上面的按钮点击事件相同。\n","            msg.submit(chat_with_db, inputs=[msg, chatbot, history_len, top_k, temperature], outputs=[msg, chatbot]) \n","\n","            with gr.Row():\n","                # 创建一个提交按钮。\n","                btn = gr.Button(\"Submit\")\n","                # 创建一个清除按钮，用于清除文本框和聊天机器人组件的内容。\n","                clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n","    \n","        with gr.Column(scale=1):\n","            model_argument = gr.Accordion(\"问答参数配置\", )\n","            with model_argument:\n","                top_k = gr.Slider(1,\n","                                10,\n","                                value=3,\n","                                step=1,\n","                                label=\"vector db search top k\",\n","                                interactive=True)\n","\n","                history_len = gr.Slider(0,\n","                                        5,\n","                                        value=3,\n","                                        step=1,\n","                                        label=\"history len\",\n","                                        interactive=True)\n","\n","                temperature = gr.Slider(0,\n","                                        1,\n","                                        value=0.01,\n","                                        step=0.01,\n","                                        label=\"llm temperature\",\n","                                        interactive=True)\n","\n","            file = gr.File(label='请上传知识库文件',\n","                            file_types=['.txt', '.md', '.docx', '.pdf'])\n","            with gr.Row():\n","                # # 创建一个提交按钮。\n","                # btn = gr.Button(\"Submit\")\n","                # # 创建一个清除按钮，用于清除文本框和聊天机器人组件的内容。\n","                # clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n","                init_db = gr.Button(\"知识库文件向量化\")\n","                \n","        init_db.click(\n","            init_knowledge_db,\n","            show_progress=True,\n","            inputs=[file],\n","            outputs=[],\n","        )\n","\n","        # 设置按钮的点击事件。当点击时，调用上面定义的 chat_with_db 函数，并传入用户的消息和聊天历史记录，然后更新文本框和聊天机器人组件。\n","        btn.click(chat_with_db, inputs=[msg, chatbot, history_len, top_k, temperature], outputs=[msg, chatbot])\n","        # 设置文本框的提交事件（即按下Enter键时）。功能与上面的按钮点击事件相同。\n","        msg.submit(chat_with_db, inputs=[msg, chatbot, history_len, top_k, temperature], outputs=[msg, chatbot]) \n","\n","    gr.Markdown(\"\"\"提醒：<br>\n","    1. 使用时请先上传自己的知识文件，并且文件中不含某些特殊字符，否则将返回error. <br>\n","    \"\"\")\n","# threads to consume the request\n","gr.close_all()\n","# 启动新的Gradio应用，设置分享功能为True，并使用环境变量PORT3指定服务器端口。\n","# demo.launch(share=True, server_port=int(os.environ['PORT3']))\n","demo.launch()"]},{"cell_type":"markdown","id":"cf2fb145","metadata":{},"source":["现在我们已经实现了 《llm 通过本地数据库进行回答》的基本功能和界面。快去进行自己的尝试吧。\n","\n","后续还可以增加很多功能优化：\n","  1. 增加模型选择功能\n","  2. embedding 选择功能\n","  3. 更细致化的模型配置"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":5}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文档加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用 PyMuPDFLoader 来读取知识库的 PDF 文件。PyMuPDFLoader 是 PDF 解析器中速度最快的一种，结果会包含 PDF 及其页面的详细元数据，并且每页返回一个文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 安装必要的库\n",
    "# !pip install rapidocr_onnxruntime -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip install \"unstructured[all-docs]\" -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip install pyMuPDF -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# 创建一个 PyMuPDFLoader Class 实例，输入为待加载的 pdf 文档路径\n",
    "loader = PyMuPDFLoader(\"../docs/LeeDL-Tutorial/LeeDL_Tutorial.pdf\")\n",
    "\n",
    "# 调用 PyMuPDFLoader Class 的函数 load 对 pdf 文件进行加载\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 探索加载的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文档加载后储存在`pages`变量中:\n",
    "- `page`的变量类型为`List`\n",
    "- 打印 `pages` 的长度可以看到pdf一共包含多少页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "载入后的变量类型为：<class 'list'>， 该 PDF 一共包含 330 页\n"
     ]
    }
   ],
   "source": [
    "print(f\"载入后的变量类型为：{type(pages)}，\",  f\"该 PDF 一共包含 {len(pages)} 页\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`page`中的每一元素为一个文档，变量类型为`langchain.schema.document.Document`, 文档变量类型包含两个属性\n",
    "- `page_content` 包含该文档的内容。\n",
    "- `meta_data` 为文档相关的描述性数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每一个元素的类型：<class 'langchain.schema.document.Document'>.\n",
      "\n",
      "该文档的描述性数据：{'source': '../docs/LeeDL-Tutorial/LeeDL_Tutorial.pdf', 'file_path': '../docs/LeeDL-Tutorial/LeeDL_Tutorial.pdf', 'page': 8, 'total_pages': 330, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': \"D:20230831225119+08'00'\", 'modDate': '', 'trapped': ''}\n",
      "\n",
      "查看该文档的内容：→_→\n",
      "https://github.com/datawhalechina/leedl-tutorial\n",
      "←_←\n",
      "第 1 章\n",
      "机器学习基础\n",
      "首先简单介绍一下机器学习（machine learning）和深度学习（deep learning）的基本概\n",
      "念。机器学习，顾名思义，机器具备有学习的能力。具体来讲，机器学习就是让机器具备找一\n",
      "个函数的能力。机器具备找函数的能力以后，它可以做很多事，举个例子：\n",
      "• 语音识别：机器听一段声音，产生这段声音对应的文字。我们需要的是一个函数，该函\n",
      "数的输入是声音信号，输出是这段声音信号的内容。这个函数显然非常复杂，人类难以\n",
      "把它写出来，因此想通过机器的力量把这个函数自动找出来。\n",
      "• 还有好多的任务需要找一个很复杂的函数，以图像识别为例，图像识别函数的输入是一\n",
      "张图片，输出是这个图片里面的内容。\n",
      "• AlphaGo 也可以看作是一个函数，机器下围棋需要的就是一个函数，该函数的输入是棋\n",
      "盘上黑子跟白子的位置。输出是机器下一步应该落子的位置。\n",
      "随着要找的函数不同，机器学习有不同的类别。假设要找的函数的输出是一个数值，一个\n",
      "标量（scalar），这种机器学习的任\n"
     ]
    }
   ],
   "source": [
    "page = pages[8]\n",
    "print(f\"每一个元素的类型：{type(page)}.\", \n",
    "    f\"该文档的描述性数据：{page.metadata}\", \n",
    "    f\"查看该文档的内容：{page.page_content[0:500]}\", \n",
    "    sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到我们成功导入了我们所需的文档。接下来我们将文档进行切分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、文档分割方式\n",
    "Langchain 中文本分割器都根据 chunk_size (块大小)和 chunk_overlap (块与块之间的重叠大小)进行分割。\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "* chunk_size 指每个块包含的字符或 Token（如单词、句子等）的数量\n",
    "\n",
    "* chunk_overlap 指两个块之间共享的字符数量，用于保持上下文的连贯性，避免分割丢失上下文信息\n",
    "\n",
    "![image.png](attachment:image-2.png)\n",
    "\n",
    "Langchain 提供多种文档分割方式，区别在怎么确定块与块之间的边界、块由哪些字符/token组成、以及如何测量块大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RecursiveCharacterTextSplitter():按字符串分割文本，递归地尝试按不同的分隔符进行分割文本。\n",
    "- CharacterTextSplitter()：按字符来分割文本。\n",
    "- MarkdownHeaderTextSplitter()：基于指定的标题来分割markdown 文件。\n",
    "- TokenTextSplitter()：按token来分割文本。\n",
    "- SentenceTransformersTokenTextSplitter() : 按token来分割文本\n",
    "- Language() - 用于 CPP、Python、Ruby、Markdown 等。\n",
    "- NLTKTextSplitter()：使用 NLTK（自然语言工具包）按句子分割文本。\n",
    "- SpacyTextSplitter() - 使用 Spacy按句子的切割文本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 基于token的分割\n",
    "很多 LLM 的上下文窗口长度限制是按照 Token 来计数的。因此，以 LLM 的视角，按照 Token 对文本进行分隔，通常可以得到更好的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 知识库中单段文本长度\n",
    "CHUNK_SIZE = 250\n",
    "\n",
    "# 知识库中相邻文本重合长度\n",
    "OVERLAP_SIZE = 50\n",
    "\n",
    "# 知识库匹配向量数量\n",
    "VECTOR_SEARCH_TOP_K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['→_→\\nhttps://github.com/datawhalechina/leedl-tutorial\\n←_←\\n第 1 章\\n机器学习基础\\n首先简单介绍一下机器学习（machine learning）和深度学习（deep learning）的基本概\\n念。机器学习，顾名思义，机器具备有学习的能力。具体来讲，机器学习就是让机器具备找一\\n个函数的能力。机器具备找函数的能力以后，它可',\n",
       " '以做很多事，举个例子：\\n• 语音识别：机器听一段声音，产生这段声音对应的文字。我们需要的是一个函数，该函\\n数的输入是声音信号，输出是这段声音信号的内容。这个函数显然非常复杂，人类难以\\n把它写出来，因此想通过机器的力量把这个函数自动找出来。\\n• 还',\n",
       " '有好多的任务需要找一个很复杂的函数，以图像识别为例，图像识别函数的输入是一\\n张图片，输出是这个图片里面的内容。\\n• AlphaGo 也可以看作是一个函数，机器下围棋需要的就是一个函数，该函数的输入是棋\\n盘上黑子跟白子的位置。输出是机器下一步应该落子的位置。\\n随着要找的函数不',\n",
       " '同，机器学习有不同的类别。假设要找的函数的输出是一个数值，一个\\n标量（scalar），这种机器学习的任']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用token分割器进行分割，\n",
    "# 将块大小设为1，块重叠大小设为0，相当于将任意字符串分割成了单个Token组成的列\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\n",
    "\n",
    "text_splitter.split_text(page.page_content[0:500])\n",
    "\n",
    "# loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "切分后的文件数量：2236\n"
     ]
    }
   ],
   "source": [
    "split_docs = text_splitter.split_documents(pages)\n",
    "print(f\"切分后的文件数量：{len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、Embeddings\n",
    "\n",
    "什么是`Embeddings`？\n",
    "\n",
    "\n",
    "在机器学习和自然语言处理（NLP）中，`Embeddings`（嵌入）是一种将类别数据，如单词、句子或者整个文档，转化为实数向量的技术。这些实数向量可以被计算机更好地理解和处理。嵌入背后的主要想法是，相似或相关的对象在嵌入空间中的距离应该很近。\n",
    "\n",
    "举个例子，我们可以使用词嵌入（word embeddings）来表示文本数据。在词嵌入中，每个单词被转换为一个向量，这个向量捕获了这个单词的语义信息。例如，\"king\" 和 \"queen\" 这两个单词在嵌入空间中的位置将会非常接近，因为它们的含义相似。而 \"apple\" 和 \"orange\" 也会很接近，因为它们都是水果。而 \"king\" 和 \"apple\" 这两个单词在嵌入空间中的距离就会比较远，因为它们的含义不同。\n",
    "\n",
    "让我们取出我们的切分部分并对它们进行`Embedding`处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里提供两种方式进行，一种是直接使用 openai 的模型去生成 embedding，另一种是使用 HuggingFace 上的模型去生成 embedding。\n",
    "\n",
    "- openAI 的模型需要消耗 api，对于大量的token 来说成本会比较高，但是非常方便。\n",
    "- HuggingFace 的模型可以本地部署，可自定义合适的模型，可玩性较高，但对本地的资源有部分要求。\n",
    "\n",
    "\n",
    "对于只想体验一下的同学来说，可以尝试直接用生成好的 embedding，或者在本地部署小模型进行尝试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace 是一个优秀的开源库，我们只需要输入模型的名字，就会自动帮我们解析对应的能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"moka-ai/m3e-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HuggingFaceEmbeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbeddings\u001b[49m(model_name\u001b[38;5;241m=\u001b[39membedding_model_dict[model], model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: device})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HuggingFaceEmbeddings' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings() \n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m query1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m什么是机器学习\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m emb1 \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241m.\u001b[39membed_query(query1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding' is not defined"
     ]
    }
   ],
   "source": [
    "query1 = \"什么是机器学习\"\n",
    "query2 = \"什么是强化学习\"\n",
    "query2 = \"什么是独立学习\"\n",
    "\n",
    "emb1 = embedding.embed_query(query1)\n",
    "emb2 = embedding.embed_query(query1)\n",
    "emb3 = embedding.embed_query(query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "什么是机器学习\n"
     ]
    }
   ],
   "source": [
    "print(f\"{query1} 对应的 embedding： {emb1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常，通过点积查看两个向量的结果来判断向量之间的相似度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

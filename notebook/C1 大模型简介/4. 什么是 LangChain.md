
ChatGPT的巨大成功激发了越来越多的开发者兴趣，他们希望利用OpenAI提供的API或者私有化模型，来开发基于大型语言模型的应用程序。尽管大型语言模型的调用相对简单，但要创建完整的应用程序，仍然需要大量的定制开发工作，包括API集成、互动逻辑、数据存储等等。

为了解决这个问题，从2022年开始，许多机构和个人相继推出了多个开源项目，旨在帮助开发者们迅速构建基于大型语言模型的端到端应用程序或工作流程。其中一个备受关注的项目就是LangChain框架。LangChain框架是一个开源工具，充分利用了大型语言模型的强大能力，以便开发各种下游应用。它的目标是为各种大型语言模型应用提供通用接口，从而简化应用程序的开发流程。具体来说，LangChain框架可以实现数据感知和环境互动，也就是说，它能够让语言模型与其他数据来源连接，并且允许语言模型与其所处的环境进行互动。

在接下来的部分中，我们将重点介绍 LangChain 框架的核心模块，带您深入了解这个令人兴奋的工具。

本模块将重点介绍 LangChain 的 6 种标准化、可扩展的接口并且可以外部集成的核心模块:

- **模型输入/输出（Model I/O）**：与语言模型交互的接口
- **数据连接（Data connection）**：与特定应用程序的数据进行交互的接
- **链(Chains)**：将组件组合实现端到端应用。
- **记忆（Memory）**：用于链的多次运行之间持久化应用程序状态；
- **索引(Indexes)**：提供数据检索功能。
- **代理(Agents)**：扩展模型的推理能力。


，用于复杂的应用的调用序列；
智能体（Agents），语言模型作为推理器决定要执行的动作序列；

回调（Callbacks）记录和流式传输任何链式组装的中间步骤

## 1. 模型输入/输出

LangChain 中模型输入/输出模块是与各种大语言模型进行交互的基本组件，是大语言模型应用的核心元素。该模块的基本流程如下图所示。

![](../../figures/langchain_model_input_output.png)

主要包含以下部分：`Prompts`、`Language Models`以及 `Output Parsers`。用户原始输入与模型和示例进行组合，然后输入给大语言模型，再根据大语言模型的返回结果进行输出或者结构化处理。

我们首先向您演示直接调用 OpenAI 的场景，以充分说明为什么我们需要使用 LangChain。

### 1.1、直接调用OpenAI

#### 1.1.1 计算1+1

我们来看一个简单的例子，直接使用通过 OpenAI 接口封装的函数`get_completion`来让模型告诉我们：`1+1是什么？`


```python
from tool import get_completion

get_completion("1+1是什么？")
```

    '1+1等于2。'

#### 1.1.2 用普通话表达海盗邮件

在上述简单示例中，模型`gpt-3.5-turbo`为我们提供了关于1+1是什么的答案。而现在，我们进入一个更为丰富和复杂的场景。

设想一下，你是一家电商公司的员工。你们的客户中有一位名为海盗A的特殊顾客。他在你们的平台上购买了一个榨汁机，目的是为了制作美味的奶昔。但在制作过程中，由于某种原因，奶昔的盖子突然弹开，导致厨房的墙上洒满了奶昔。想象一下这名海盗的愤怒和挫败之情。他用充满海盗特色的英语方言，给你们的客服中心写了一封邮件：`customer_email`。


```python
customer_email = """
嗯呐，我现在可是火冒三丈，我那个搅拌机盖子竟然飞了出去，把我厨房的墙壁都溅上了果汁！
更糟糕的是，保修条款可不包括清理我厨房的费用。
伙计，赶紧给我过来！
"""
```

在处理来自多元文化背景的顾客时，我们的客服团队可能会遇到某些特殊的语言障碍。如上，我们收到了一名海盗客户的邮件，而他的表达方式对于我们的客服团队来说略显生涩。

为了解决这一挑战，我们设定了以下两个目标：

- 首先，我们希望模型能够将这封充满海盗方言的邮件翻译成普通话，这样客服团队就能更容易地理解其内容。
- 其次，在进行翻译时，我们期望模型能采用平和和尊重的语气，这不仅能确保信息准确传达，还能保持与顾客之间的和谐关系。

为了指导模型的输出，我们定义了一个文本表达风格标签，简称为`style`。


```python
# 普通话 + 平静、尊敬的语调
style = """正式普通话 \
用一个平静、尊敬、有礼貌的语调
"""
```

下一步我们需要做的是将`customer_email`和`style`结合起来构造我们的提示:`prompt`


```python
# 要求模型根据给出的语调进行转化
prompt = f"""把由三个反引号分隔的文本\
翻译成一种{style}风格。
文本: ```{customer_email}```
"""

print("提示：", prompt)
```

    
    提示： 
     把由三个反引号分隔的文本翻译成一种正式普通话 用一个平静、尊敬、有礼貌的语调
    风格。
    文本: ```
    嗯呐，我现在可是火冒三丈，我那个搅拌机盖子竟然飞了出去，把我厨房的墙壁都溅上了果汁！
    更糟糕的是，保修条款可不包括清理我厨房的费用。
    伙计，赶紧给我过来！
    ```
    


经过精心设计的`prompt`已经准备就绪。接下来，只需调用`get_completion`方法，我们就可以获得期望的输出——那封原汁原味的海盗方言邮件，将被翻译成既平和又尊重的正式普通话表达。


```python
response = get_completion(prompt)
print(response)
```

    非常抱歉，我现在感到非常愤怒和不满。我的搅拌机盖子竟然飞了出去，导致我厨房的墙壁上都溅满了果汁！更糟糕的是，保修条款并不包括清理我厨房的费用。先生/女士，请您尽快过来处理这个问题！


在进行语言风格转换之后，我们可以观察到明显的变化：原本的用词变得更为正式，那些带有极端情绪的表达得到了替代，并且文本中还加入了表示感激的词汇。

> 小建议：你可以调整并尝试不同的提示，来探索模型能为你带来怎样的创新性输出。每次尝试都可能为你带来意想不到的惊喜！

### 1.2 通过LangChain使用OpenAI

在前面的小节中，我们使用了封装好的函数`get_completion`，利用 OpenAI 接口成功地对那封充满方言特色的邮件进行了翻译。得到一封采用平和且尊重的语气、并用标准普通话所写的邮件。接下来，我们将尝试使用 LangChain 解决该问题。


#### 1.2.1 模型 

现在让我们尝试使用LangChain来实现相同的功能。从`langchain.chat_models`导入`OpenAI`的对话模型`ChatOpenAI`。 除去OpenAI以外，`langchain.chat_models`还集成了其他对话模型，更多细节可以查看 [Langchain 官方文档](https://python.langchain.com/en/latest/modules/models/chat/integrations.html)(https://python.langchain.com/en/latest/modules/models/chat/integrations.html)。


```python
from langchain.chat_models import ChatOpenAI

# 这里我们将参数temperature设置为0.0，从而减少生成答案的随机性。
# 如果你想要每次得到不一样的有新意的答案，可以尝试调整该参数。
chat = ChatOpenAI(temperature=0.0)
chat
```


    ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None)



<br>

上面的输出显示ChatOpenAI的默认模型为`gpt-3.5-turbo`

#### 1.2.2 使用提示模版

在前面的例子中，我们通过[f字符串](https://docs.python.org/zh-cn/3/tutorial/inputoutput.html#tut-f-strings)把Python表达式的值`style`和`customer_email`添加到`prompt`字符串内。

`langchain`提供了接口方便快速的构造和使用提示。

##### 1.2.2.1 用普通话表达海盗邮件

现在我们来看看如何使用`langchain`来构造提示吧！


```python
from langchain.prompts import ChatPromptTemplate

# 首先，构造一个提示模版字符串：`template_string`
template_string = """把由三个反引号分隔的文本\
翻译成一种{style}风格。\
文本: ```{text}```
"""

# 然后，我们调用`ChatPromptTemplatee.from_template()`函数将
# 上面的提示模版字符`template_string`转换为提示模版`prompt_template`

prompt_template = ChatPromptTemplate.from_template(template_string)


print("\n", prompt_template.messages[0].prompt)
```

    
     input_variables=['style', 'text'] output_parser=None partial_variables={} template='把由三个反引号分隔的文本翻译成一种{style}风格。文本: ```{text}```\n' template_format='f-string' validate_template=True


<br>

对于给定的`customer_style`和`customer_email`, 我们可以使用提示模版`prompt_template`的`format_messages`方法生成想要的客户消息`customer_messages`。

提示模版`prompt_template`需要两个输入变量： `style` 和 `text`。 这里分别对应 
- `customer_style`: 我们想要的顾客邮件风格
- `customer_email`: 顾客的原始邮件文本。


```python
customer_style = """正式普通话 \
用一个平静、尊敬的语气
"""

customer_email = """
嗯呐，我现在可是火冒三丈，我那个搅拌机盖子竟然飞了出去，把我厨房的墙壁都溅上了果汁！
更糟糕的是，保修条款可不包括清理我厨房的费用。
伙计，赶紧给我过来！
"""

# 使用提示模版
customer_messages = prompt_template.format_messages(
                    style=customer_style,
                    text=customer_email)
# 打印客户消息类型
print("客户消息类型:",type(customer_messages),"\n")

# 打印第一个客户消息类型
print("第一个客户客户消息类型类型:", type(customer_messages[0]),"\n")

# 打印第一个元素
print("第一个客户客户消息类型类型: ", customer_messages[0],"\n")

```

    客户消息类型:
     <class 'list'> 
    
    第一个客户客户消息类型类型:
     <class 'langchain.schema.messages.HumanMessage'> 
    
    第一个客户客户消息类型类型: 
     content='把由三个反引号分隔的文本翻译成一种正式普通话 用一个平静、尊敬的语气\n风格。文本: ```\n嗯呐，我现在可是火冒三丈，我那个搅拌机盖子竟然飞了出去，把我厨房的墙壁都溅上了果汁！\n更糟糕的是，保修条款可不包括清理我厨房的费用。\n伙计，赶紧给我过来！\n```\n' additional_kwargs={} example=False 
    


<br>

可以看出
- `customer_messages`变量类型为列表(`list`)
- 列表里的元素变量类型为langchain自定义消息(`langchain.schema.HumanMessage`)。

<br>

现在我们可以调用模型部分定义的`chat`模型来实现转换客户消息风格。


```python
customer_response = chat(customer_messages)
print(customer_response.content)
```

    非常抱歉，我现在感到非常愤怒。我的搅拌机盖子竟然飞了出去，导致我厨房的墙壁上都溅满了果汁！更糟糕的是，保修条款并不包括清理我厨房的费用。伙计，请你尽快过来帮我解决这个问题！


##### 1.2.2.2 用海盗方言回复邮件

到目前为止，我们已经实现了在前一部分的任务。接下来，我们更进一步，将客服人员回复的消息，转换为海盗风格英语，并确保消息比较有礼貌。 这里，我们可以继续使用起前面构造的的langchain提示模版，来获得我们回复消息提示。


```python
service_reply = """嘿，顾客， \
保修不包括厨房的清洁费用， \
因为您在启动搅拌机之前 \
忘记盖上盖子而误用搅拌机, \
这是您的错。 \
倒霉！ 再见！
"""

service_style_pirate = """\
一个有礼貌的语气 \
使用海盗风格\
"""
service_messages = prompt_template.format_messages(
    style=service_style_pirate,
    text=service_reply)

print("\n", service_messages[0].content)
```

    
     把由三个反引号分隔的文本翻译成一种一个有礼貌的语气 使用海盗风格风格。文本: ```嘿，顾客， 保修不包括厨房的清洁费用， 因为您在启动搅拌机之前 忘记盖上盖子而误用搅拌机, 这是您的错。 倒霉！ 再见！
    ```
    



```python
# 调用模型部分定义的chat模型来转换回复消息风格
service_response = chat(service_messages)
print(service_response.content)
```

    嘿，尊贵的客户啊，保修可不包括厨房的清洁费用，因为您在启动搅拌机之前竟然忘记盖上盖子而误用了搅拌机，这可是您的疏忽之过啊。真是倒霉透顶啊！祝您一路顺风！


##### 1.2.2.3 为什么需要提示模版

在应用于比较复杂的场景时，提示可能会非常长并且包含涉及许多细节。**使用提示模版，可以让我们更为方便地重复使用设计好的提示**。英文版提示2.2.3 给出了作业的提示模版案例：学生们线上学习并提交作业，通过提示来实现对学生的提交的作业的评分。

此外，LangChain还提供了提示模版用于一些常用场景。比如自动摘要、问答、连接到SQL数据库、连接到不同的API。通过使用LangChain内置的提示模版，你可以快速建立自己的大模型应用，而不需要花时间去设计和构造提示。

最后，我们在建立大模型应用时，通常希望模型的输出为给定的格式，比如在输出使用特定的关键词来让输出结构化。英文版提示2.2.3 给出了使用大模型进行链式思考推理结果示例 -- 对于问题：*What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?* 通过使用LangChain库函数，输出采用"Thought"（思考）、"Action"（行动）、"Observation"（观察）作为链式思考推理的关键词，让输出结构化。

#### 1.2.3 输出解析器

##### 1.2.3.1 不使用输出解释器提取客户评价中的信息

对于给定的评价`customer_review`, 我们希望提取信息，并按以下格式输出：

```json
{
  "gift": False,
  "delivery_days": 5,
  "price_value": "pretty affordable!"
}
```


```python
from langchain.prompts import ChatPromptTemplate

customer_review = """\
这款吹叶机非常神奇。 它有四个设置：\
吹蜡烛、微风、风城、龙卷风。 \
两天后就到了，正好赶上我妻子的\
周年纪念礼物。 \
我想我的妻子会喜欢它到说不出话来。 \
到目前为止，我是唯一一个使用它的人，而且我一直\
每隔一天早上用它来清理草坪上的叶子。 \
它比其他吹叶机稍微贵一点，\
但我认为它的额外功能是值得的。
"""

review_template = """\
对于以下文本，请从中提取以下信息：

礼物：该商品是作为礼物送给别人的吗？ \
如果是，则回答 是的；如果否或未知，则回答 不是。

交货天数：产品需要多少天\
到达？ 如果没有找到该信息，则输出-1。

价钱：提取有关价值或价格的任何句子，\
并将它们输出为逗号分隔的 Python 列表。

使用以下键将输出格式化为 JSON：
礼物
交货天数
价钱

文本: {text}
"""

prompt_template = ChatPromptTemplate.from_template(review_template)
print("提示模版：", prompt_template)


messages = prompt_template.format_messages(text=customer_review)


chat = ChatOpenAI(temperature=0.0)
response = chat(messages)

print("结果类型:", type(response.content))
print("结果:", response.content)
```

    
    提示模版： 
     input_variables=['text'] output_parser=None partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], output_parser=None, partial_variables={}, template='对于以下文本，请从中提取以下信息：\n\n礼物：该商品是作为礼物送给别人的吗？ 如果是，则回答 是的；如果否或未知，则回答 不是。\n\n交货天数：产品需要多少天到达？ 如果没有找到该信息，则输出-1。\n\n价钱：提取有关价值或价格的任何句子，并将它们输出为逗号分隔的 Python 列表。\n\n使用以下键将输出格式化为 JSON：\n礼物\n交货天数\n价钱\n\n文本: {text}\n', template_format='f-string', validate_template=True), additional_kwargs={})]
    
    结果类型:
     <class 'str'>
    
    结果:
     {
      "礼物": "是的",
      "交货天数": 2,
      "价钱": ["它比其他吹叶机稍微贵一点"]
    }


可以看出 `response.content`类型为字符串（`str`），而并非字典(`dict`), 如果想要从中更方便的提取信息，我们需要使用`Langchain`中的输出解释器。

##### 1.2.3.2 使用输出解析器提取客户评价中的信息

接下来，我们将展示如何使用输出解释器。


```python
review_template_2 = """\
对于以下文本，请从中提取以下信息：：

礼物：该商品是作为礼物送给别人的吗？
如果是，则回答 是的；如果否或未知，则回答 不是。

交货天数：产品到达需要多少天？ 如果没有找到该信息，则输出-1。

价钱：提取有关价值或价格的任何句子，并将它们输出为逗号分隔的 Python 列表。

文本: {text}

{format_instructions}
"""

prompt = ChatPromptTemplate.from_template(template=review_template_2)

from langchain.output_parsers import ResponseSchema
from langchain.output_parsers import StructuredOutputParser

gift_schema = ResponseSchema(name="礼物",
                             description="这件物品是作为礼物送给别人的吗？\
                            如果是，则回答 是的，\
                            如果否或未知，则回答 不是。")

delivery_days_schema = ResponseSchema(name="交货天数",
                                      description="产品需要多少天才能到达？\
                                      如果没有找到该信息，则输出-1。")

price_value_schema = ResponseSchema(name="价钱",
                                    description="提取有关价值或价格的任何句子，\
                                    并将它们输出为逗号分隔的 Python 列表")


response_schemas = [gift_schema, 
                    delivery_days_schema,
                    price_value_schema]
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
format_instructions = output_parser.get_format_instructions()
print("输出格式规定：",format_instructions)
```

    
    输出格式规定： 
     The output should be a markdown code snippet formatted in the following schema, including the leading and trailing "```json" and "```":
    
    ```json
    {
    	"礼物": string  // 这件物品是作为礼物送给别人的吗？                            如果是，则回答 是的，                            如果否或未知，则回答 不是。
    	"交货天数": string  // 产品需要多少天才能到达？                                      如果没有找到该信息，则输出-1。
    	"价钱": string  // 提取有关价值或价格的任何句子，                                    并将它们输出为逗号分隔的 Python 列表
    }
    ```



```python
messages = prompt.format_messages(text=customer_review, format_instructions=format_instructions)
print("第一条客户消息:",messages[0].content)
```

    第一条客户消息:
     对于以下文本，请从中提取以下信息：：
    
    礼物：该商品是作为礼物送给别人的吗？
    如果是，则回答 是的；如果否或未知，则回答 不是。
    
    交货天数：产品到达需要多少天？ 如果没有找到该信息，则输出-1。
    
    价钱：提取有关价值或价格的任何句子，并将它们输出为逗号分隔的 Python 列表。
    
    文本: 这款吹叶机非常神奇。 它有四个设置：吹蜡烛、微风、风城、龙卷风。 两天后就到了，正好赶上我妻子的周年纪念礼物。 我想我的妻子会喜欢它到说不出话来。 到目前为止，我是唯一一个使用它的人，而且我一直每隔一天早上用它来清理草坪上的叶子。 它比其他吹叶机稍微贵一点，但我认为它的额外功能是值得的。
    
    
    The output should be a markdown code snippet formatted in the following schema, including the leading and trailing "```json" and "```":
    
    ```json
    {
    	"礼物": string  // 这件物品是作为礼物送给别人的吗？                            如果是，则回答 是的，                            如果否或未知，则回答 不是。
    	"交货天数": string  // 产品需要多少天才能到达？                                      如果没有找到该信息，则输出-1。
    	"价钱": string  // 提取有关价值或价格的任何句子，                                    并将它们输出为逗号分隔的 Python 列表
    }
    ```
    



```python
response = chat(messages)

print("结果类型:", type(response.content))
print("结果:", response.content)
```

    
    结果类型:
     <class 'str'>
    
    结果:
     ```json
    {
    	"礼物": "不是",
    	"交货天数": "两天后就到了",
    	"价钱": "它比其他吹叶机稍微贵一点"
    }
    ```



```python
output_dict = output_parser.parse(response.content)

print("解析后的结果类型:", type(output_dict))
print("解析后的结果:", output_dict)
```

    
    解析后的结果类型:
     <class 'dict'>
    
    解析后的结果:
     {'礼物': '不是', '交货天数': '两天后就到了', '价钱': '它比其他吹叶机稍微贵一点'}


`output_dict`类型为字典(`dict`), 可直接使用`get`方法。这样的输出更方便下游任务的处理。

## 2. 数据连接

大语言模型(Large Language Model, LLM), 比如 ChatGPT , 可以回答许多不同的问题。但是大语言模型的知识来源于其训练数据集，并没有用户的信息（比如用户的个人数据，公司的自有数据），也没有最新发生时事的信息（在大模型数据训练后发表的文章或者新闻）。因此大模型能给出的答案比较受限。如果能够让大模型在训练数据集的基础上，利用我们自有数据中的信息来回答我们的问题，那便能够得到更有用的答案。

为了支持上述应用的构建，LangChain 数据连接（Data connection）模块通过以下方式提供组件来加载、转换、存储和查询数据：Document loaders、Document transformers、Text embedding models、Vector stores 以及 Retrievers。数据连接模块部分的基本框架如下图所示。

![](../../figures/data_collection.png)

### 2.1 文档加载

Document loaders（文档加载）旨在从源中加载数据构建 Document。LangChain 中 Document是包含文本和与其关联的元数据。LangChain 中包含加载简单 txt 文件的文档加载器，用于加载任何网页的文本内容的加载器，甚至还包含用于加载 YouTube 视频的转录稿的加载器。以下是一个最简单的从文件中读取文本加载数据的 Document 的示例：




```python
!pip install -q pypdf #注意，要运行以下代码，你需要安装第三方库 pypdf

from langchain.document_loaders import PyPDFLoader

# 创建一个 PyPDFLoader Class 实例，输入为待加载的pdf文档路径
loader = PyPDFLoader("docs/matplotlib/第一回：Matplotlib初相识.pdf")

# 调用 PyPDFLoader Class 的函数 load对pdf文件进行加载
pages = loader.load()
```

一旦文档被加载，它会被存储在名为`pages`的变量里。此外，`pages`的数据结构是一个`List`类型。为了确认其类型，我们可以借助Python内建的`type`函数来查看`pages`的确切数据类型。


```python
print(type(pages))
```

    <class 'list'>


通过输出 `pages` 的长度，我们可以轻松地了解该PDF文件包含的总页数。


```python
print(len(pages))
```

    3


在`page`变量中，每一个元素都代表一个文档，它们的数据类型是`langchain.schema.Document`。


```python
page = pages[0]
print(type(page))
```

    <class 'langchain.schema.document.Document'>


`langchain.schema.Document`类型包含两个属性：

1. `page_content`：包含该文档页面的内容。



```python
print(page.page_content[0:500])
```

    第⼀回：Matplotlib 初相识
    ⼀、认识matplotlib
    Matplotlib 是⼀个 Python 2D 绘图库，能够以多种硬拷⻉格式和跨平台的交互式环境⽣成出版物质量的图形，⽤来绘制各种静态，动态，
    交互式的图表。
    Matplotlib 可⽤于 Python 脚本， Python 和 IPython Shell 、 Jupyter notebook ， Web 应⽤程序服务器和各种图形⽤户界⾯⼯具包等。
    Matplotlib 是 Python 数据可视化库中的泰⽃，它已经成为 python 中公认的数据可视化⼯具，我们所熟知的 pandas 和 seaborn 的绘图接⼝
    其实也是基于 matplotlib 所作的⾼级封装。
    为了对matplotlib 有更好的理解，让我们从⼀些最基本的概念开始认识它，再逐渐过渡到⼀些⾼级技巧中。
    ⼆、⼀个最简单的绘图例⼦
    Matplotlib 的图像是画在 figure （如 windows ， jupyter 窗体）上的，每⼀个 figure ⼜包含了⼀个或多个 axes （⼀个可以指定坐标系的⼦区
    域）。最简单的创建 figure 


2. `meta_data`：为文档页面相关的描述性数据。


```python
print(page.metadata)
```

    {'source': 'docs/matplotlib/第一回：Matplotlib初相识.pdf', 'page': 0}


### 2.2 文档转换

Document transformers（文档转换）旨在处理文档，以完成各种转换任务，如将文档格式化为Q&A 形式，去除文档中的冗余内容等，从而更好地满足不同应用程序的需求。一个简单的文档转换示例是将长文档分割成较小的部分，以适应不同模型的上下文窗口大小。LangChain 中有许多内置的文档转换器，使拆分、合并、过滤和其他操作文档变得很容易。以下是对长文档进行拆分的代码示例：

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
# This is a long document we can split up.
with open('../../state_of_the_union.txt') as f:
text_splitter = RecursiveCharacterTextSplitter(
# Set a really small chunk size, just to show.
chunk_size = 100,
chunk_overlap = 20,
length_function = len,
add_start_index = True,
)
texts = text_splitter.create_documents([state_of_the_union])
print(texts[0])
print(texts[1])
```

根据上例可以获得如下输出结果：
>page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman.
>Members of Congress and' metadata={'start_index': 0}
>page_content='of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.'
>metadata={'start_index': 82}

### 2.3 文本嵌入模型

什么是`Embeddings`？

在机器学习和自然语言处理（NLP）中，`Embeddings`（嵌入）是一种将类别数据，如单词、句子或者整个文档，转化为实数向量的技术。这些实数向量可以被计算机更好地理解和处理。嵌入背后的主要想法是，相似或相关的对象在嵌入空间中的距离应该很近。

举个例子，我们可以使用词嵌入（word embeddings）来表示文本数据。在词嵌入中，每个单词被转换为一个向量，这个向量捕获了这个单词的语义信息。例如，"king" 和 "queen" 这两个单词在嵌入空间中的位置将会非常接近，因为它们的含义相似。而 "apple" 和 "orange" 也会很接近，因为它们都是水果。而 "king" 和 "apple" 这两个单词在嵌入空间中的距离就会比较远，因为它们的含义不同。

Text embedding models（文本嵌入模型）旨在将非结构化文本转换为嵌入表示。基于文本的嵌入表示，可以进行语义搜索，查找最相似的文本片段。Embeddings 类则是用于与文本嵌入模型进行交互，并为不同的嵌入模型提供统一标准接口，包括 OpenAI、Cohere 等。LangChain 中的 Embeddings类公开了两个方法：一个用于文档嵌入表示，另一个用于查询嵌入表示。前者输入多个文本，后者输入单个文本。之所以将它们作为两个单独的方法是因为某些嵌入模型为文档和查询采用了不同的嵌入策略。以下是使用 OpenAI 的 API 接口完成文本嵌入的代码示例：

```python
from langchain.embeddings import OpenAIEmbeddings
embeddings_model = OpenAIEmbeddings(openai_api_key="...")
embeddings = embeddings_model.embed_documents(
[
"Hi there!",
"Oh, hello!",
"What's your name?",
"My friends call me World",
"Hello World!"
]
)
len(embeddings), len(embeddings[0])

embedded_query = embeddings_model.embed_query("What was the name mentioned in this session?")
embedded_query[:5]
```
执行上述代码可以得到如下输出：
>(5, 1536)
[0.0053587136790156364,
-0.0004999046213924885,
0.038883671164512634,
-0.003001077566295862,
-0.00900818221271038]

### 2.4 向量存储

Vector Stores（向量存储）是存储和检索非结构化数据的主要方式之一。它首先将数据转化为嵌入表示，然后存储这些生成的嵌入向量。在查询阶段，系统会利用这些嵌入向量来检索与查询内容“最相似”的文档。向量存储的主要任务是保存这些嵌入数据并执行基于向量的搜索。Langchain集成了超过30个不同的向量存储库,如 Chroma、FAISS 和 Lance 等。由于Chroma轻量级且数据存储在内存中，这使得它非常容易启动和使用，接下来我们选择 Chroma 做示例：

首先我们指定一个持久化路径：

```python
from langchain.vectorstores import Chroma

persist_directory_chinese = 'docs/chroma/matplotlib/'
```

如果该路径存在旧的数据库文件，可以通过以下命令删除：


```python
!rm -rf './docs/chroma/matplotlib'  # 删除旧的数据库文件（如果文件夹中有文件的话）
```

接着从已加载的文档中创建一个向量数据库：


```python
vectordb_chinese = Chroma.from_documents(
    documents=splits,
    embedding=embedding,
    persist_directory=persist_directory_chinese  # 允许我们将persist_directory目录保存到磁盘上
)
```

    100%|██████████| 1/1 [00:01<00:00,  1.64s/it]


可以看到数据库长度是30，现在让我们开始使用它。

```python
print(vectordb_chinese._collection.count())
```

    27


接下来做**相似性搜索(Similarity Search)**，首先我们定义一个需要检索答案的问题：

```python
question_chinese = "Matplotlib是什么？" 
```

接着调用已加载的向量数据库根据相似性检索答案：

```python
docs_chinese = vectordb_chinese.similarity_search(question_chinese,k=3)
```

查看检索答案数量：


```python
len(docs_chinese)
```

    3

打印其 page_content 属性可以看到检索答案的文本：


```python
print(docs_chinese[0].page_content)
```

    第⼀回：Matplotlib 初相识
    ⼀、认识matplotlib
    Matplotlib 是⼀个 Python 2D 绘图库，能够以多种硬拷⻉格式和跨平台的交互式环境⽣成出版物质量的图形，⽤来绘制各种静态，动态，
    交互式的图表。
    Matplotlib 可⽤于 Python 脚本， Python 和 IPython Shell 、 Jupyter notebook ， Web 应⽤程序服务器和各种图形⽤户界⾯⼯具包等。
    Matplotlib 是 Python 数据可视化库中的泰⽃，它已经成为 python 中公认的数据可视化⼯具，我们所熟知的 pandas 和 seaborn 的绘图接⼝
    其实也是基于 matplotlib 所作的⾼级封装。
    为了对matplotlib 有更好的理解，让我们从⼀些最基本的概念开始认识它，再逐渐过渡到⼀些⾼级技巧中。
    ⼆、⼀个最简单的绘图例⼦
    Matplotlib 的图像是画在 figure （如 windows ， jupyter 窗体）上的，每⼀个 figure ⼜包含了⼀个或多个 axes （⼀个可以指定坐标系的⼦区
    域）。最简单的创建 figure 以及 axes 的⽅式是通过 pyplot.subplots命令，创建 axes 以后，可以使⽤ Axes.plot绘制最简易的折线图。
    import matplotlib.pyplot as plt
    import matplotlib as mpl
    import numpy as np
    fig, ax = plt.subplots()  # 创建⼀个包含⼀个 axes 的 figure
    ax.plot([1, 2, 3, 4], [1, 4, 2, 3]);  # 绘制图像
    Trick： 在jupyter notebook 中使⽤ matplotlib 时会发现，代码运⾏后⾃动打印出类似 <matplotlib.lines.Line2D at 0x23155916dc0>
    这样⼀段话，这是因为 matplotlib 的绘图代码默认打印出最后⼀个对象。如果不想显示这句话，有以下三种⽅法，在本章节的代码示例
    中你能找到这三种⽅法的使⽤。    

### 2.5 检索器

Retrievers（检索器）是一个接口，其功能是基于非结构化查询返回相应的文档。检索器不需要存储文档，只需要能根据查询返回结果即可。检索器可以使用向量存储的方式完成，也可以使用其他方式完成。接下来是一个基于向量
存储的检索器的代码示例：

```python
from langchain.document_loaders import TextLoader
loader = TextLoader('../../../state_of_the_union.txt')
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
embeddings = OpenAIEmbeddings()
db = FAISS.from_documents(texts, embeddings)
retriever = db.as_retriever()
docs = retriever.get_relevant_documents("what did he say about ketanji brown jackson")
```

## 3. 链

虽然独立使用大型语言模型能够应对一些简单任务，但对于更加复杂的需求，可能需要将多个大型语言模型进行链式组合，或与其他组件进行链式调用。链允许将多个组件组合在一起，创建一个单一的、连贯的应用程序。例如，可以创建一个链，接受用户输入，使用 PromptTemplate 对其进行格式化，然后将格式化后的提示词传递给大语言模型。也可以通过将多个链组合在一起或将链与其他组件组合来构建更复杂的链。

大语言模型链（LLMChain）是一个简单但非常强大的链，也是后面我们将要介绍的许多链的基础。我们以它为例，进行介绍：

```python
import warnings
warnings.filterwarnings('ignore')

from langchain.chat_models import ChatOpenAI 
from langchain.prompts import ChatPromptTemplate  
from langchain.chains import LLMChain  

# 这里我们将参数temperature设置为0.0，从而减少生成答案的随机性。
# 如果你想要每次得到不一样的有新意的答案，可以尝试调整该参数。
llm = ChatOpenAI(temperature=0.0)  

#初始化提示模版
prompt = ChatPromptTemplate.from_template("描述制造{product}的一个公司的最佳名称是什么?")

#将大语言模型(LLM)和提示（Prompt）组合成链
chain = LLMChain(llm=llm, prompt=prompt)

#运行大语言模型链
product = "大号床单套装"
chain.run(product)
```
运行输出：
>'"豪华床纺"'

除了上例中给出的 LLMChain，LangChain 中链还包含 RouterChain、SimpleSequentialChain、SequentialChain、TransformChain 等。
- RouterChain 可以根据输入数据的某些属性/特征值，选择调用不同的子链（Subchain）。
- SimpleSequentialChain 是最简单的序列链形式，其中每个步骤具有单一的输入/输出，上一个步骤的输出是下一个步骤的输入。
- SequentialChain 是简单顺序链的更复杂形式，允许多个输入/输出。
- TransformChain 可以引入自定义转换函数，对输入进行处理后进行输出。

以下是使用 SimpleSequentialChain 的代码示例：

```python
from langchain.chains import SimpleSequentialChain
llm = ChatOpenAI(temperature=0.9)

#创建两个子链

# 提示模板 1 ：这个提示将接受产品并返回最佳名称来描述该公司
first_prompt = ChatPromptTemplate.from_template(   
    "描述制造{product}的一个公司的最好的名称是什么"
)
chain_one = LLMChain(llm=llm, prompt=first_prompt)

# 提示模板 2 ：接受公司名称，然后输出该公司的长为20个单词的描述
second_prompt = ChatPromptTemplate.from_template(   
    "写一个20字的描述对于下面这个\
    公司：{company_name}的"
)
chain_two = LLMChain(llm=llm, prompt=second_prompt)

#构建简单顺序链
#现在我们可以组合两个LLMChain，以便我们可以在一个步骤中创建公司名称和描述
overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)


#运行简单顺序链
product = "大号床单套装"
overall_simple_chain.run(product)
```
运行输出：
 
    
    > Entering new SimpleSequentialChain chain...
    优床制造公司
    优床制造公司是一家专注于生产高品质床具的公司。
    
    > Finished chain.


    '优床制造公司是一家专注于生产高品质床具的公司。'


## 4. 记忆

在 LangChain 中，记忆（Memory）指的是大语言模型（LLM）的短期记忆。为什么是短期记忆？那是因为LLM训练好之后 (获得了一些长期记忆)，它的参数便不会因为用户的输入而发生改变。当用户与训练好的LLM进行对话时，LLM 会暂时记住用户的输入和它已经生成的输出，以便预测之后的输出，而模型输出完毕后，它便会“遗忘”之前用户的输入和它的输出。因此，之前的这些信息只能称作为 LLM 的短期记忆。

正如上面所说，在与语言模型交互时，你可能已经注意到一个关键问题：它们并不记忆你之前的交流内容，这在我们构建一些应用程序（如聊天机器人）的时候，带来了很大的挑战，使得对话似乎缺乏真正的连续性。因此，在本节中我们将介绍 LangChain 中的记忆模块，即如何将先前的对话嵌入到语言模型中的，使其具有连续对话的能力。

![](../../figures/memory.png)

### 4.1 ConversationBufferMemory

在 LangChain 中提供了多种记忆方式的支持，ConversationBufferMemory 是记忆中一种非常简单的形式，它只是将聊天消息列表保存到缓冲区中，并将其传递到提示模板中。代码示例如下所示：

```python
# 初始化对话模型
from langchain.chains import ConversationChain
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory

# 这里我们将参数temperature设置为0.0，从而减少生成答案的随机性。
# 如果你想要每次得到不一样的有新意的答案，可以尝试增大该参数。
llm = ChatOpenAI(temperature=0.0)  
memory = ConversationBufferMemory()

# 新建一个 ConversationChain Class 实例
# verbose参数设置为True时，程序会输出更详细的信息，以提供更多的调试或运行时信息。
# 相反，当将verbose参数设置为False时，程序会以更简洁的方式运行，只输出关键的信息。
conversation = ConversationChain(llm=llm, memory = memory, verbose=True )
```


### 4.2 第一轮对话

当我们运行预测(predict)时，生成了一些提示，如下所见，他说“以下是人类和 AI 之间友好的对话，AI 健谈“等等，这实际上是 LangChain 生成的提示，以使系统进行希望和友好的对话，并且必须保存对话，并提示了当前已完成的模型链。


```python
conversation.predict(input="你好, 我叫皮皮鲁")
```

    
    
    > Entering new  chain...
    Prompt after formatting:
    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.
    
    Current conversation:
    
    Human: 你好, 我叫皮皮鲁
    AI:
    
    > Finished chain.





    '你好，皮皮鲁！很高兴认识你。我是一个AI助手，可以回答你的问题和提供帮助。有什么我可以帮你的吗？'



### 4.3 第二轮对话

当我们进行第二轮对话时，它会保留上面的提示


```python
conversation.predict(input="1+1等于多少？")
```

    
    
    > Entering new ConversationChain chain...
    Prompt after formatting:
    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.
    
    Current conversation:
    Human: 你好, 我叫皮皮鲁
    AI: 你好，皮皮鲁！很高兴认识你。我是一个AI助手，可以回答你的问题和提供帮助。有什么我可以帮你的吗？
    Human: 1+1等于多少？
    AI:
    
    > Finished chain.





    '1+1等于2。'



### 4.4 第三轮对话

为了验证他是否记忆了前面的对话内容，我们让他回答前面已经说过的内容（我的名字），可以看到他确实输出了正确的名字，因此这个对话链随着往下进行会越来越长。


```python
conversation.predict(input="我叫什么名字？")
```

    
    
    > Entering new ConversationChain chain...
    Prompt after formatting:
    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.
    
    Current conversation:
    Human: 你好, 我叫皮皮鲁
    AI: 你好，皮皮鲁！很高兴认识你。我是一个AI助手，可以回答你的问题和提供帮助。有什么我可以帮你的吗？
    Human: 1+1等于多少？
    AI: 1+1等于2。
    Human: 我叫什么名字？
    AI:
    
    > Finished chain.





    '你叫皮皮鲁。'



### 4.5 查看储存缓存

储存缓存(buffer)，即储存了当前为止所有的对话信息

```python
print(memory.buffer) 
```

    Human: 你好, 我叫皮皮鲁
    AI: 你好，皮皮鲁！很高兴认识你。我是一个AI助手，可以回答你的问题和提供帮助。有什么我可以帮你的吗？
    Human: 1+1等于多少？
    AI: 1+1等于2。
    Human: 我叫什么名字？
    AI: 你叫皮皮鲁。


也可以通过`load_memory_variables({})`打印缓存中的历史消息。这里的`{}`是一个空字典，有一些更高级的功能，使用户可以使用更复杂的输入，具体可以通过 LangChain 的官方文档查询更高级的用法。


```python
print(memory.load_memory_variables({}))
```

    {'history': 'Human: 你好, 我叫皮皮鲁\nAI: 你好，皮皮鲁！很高兴认识你。我是一个AI助手，可以回答你的问题和提供帮助。有什么我可以帮你的吗？\nHuman: 1+1等于多少？\nAI: 1+1等于2。\nHuman: 我叫什么名字？\nAI: 你叫皮皮鲁。'}


### 4.6 直接添加内容到储存缓存

我们可以使用`save_context`来直接添加内容到`buffer`中。


```python
memory = ConversationBufferMemory()
memory.save_context({"input": "你好，我叫皮皮鲁"}, {"output": "你好啊，我叫鲁西西"})
memory.load_memory_variables({})
```




    {'history': 'Human: 你好，我叫皮皮鲁\nAI: 你好啊，我叫鲁西西'}



继续添加新的内容


```python
memory.save_context({"input": "很高兴和你成为朋友！"}, {"output": "是的，让我们一起去冒险吧！"})
memory.load_memory_variables({})
```




    {'history': 'Human: 你好，我叫皮皮鲁\nAI: 你好啊，我叫鲁西西\nHuman: 很高兴和你成为朋友！\nAI: 是的，让我们一起去冒险吧！'}



可以看到对话历史都保存下来了！

当我们在使用大型语言模型进行聊天对话时，**大型语言模型本身实际上是无状态的。语言模型本身并不记得到目前为止的历史对话**。每次调用API结点都是独立的。储存(Memory)可以储存到目前为止的所有术语或对话，并将其输入或附加上下文到LLM中用于生成输出。如此看起来就好像它在进行下一轮对话的时候，记得之前说过什么。

## 5. 代理（Agents）

大型语言模型（LLMs）非常强大，但它们缺乏“最笨”的计算机程序可以轻松处理的特定能力。LLM 对逻辑推理、计算和检索外部信息的能力较弱，这与最简单的计算机程序形成对比。例如，语言模型无法准确回答简单的计算问题，还有当询问最近发生的事件时，其回答也可能过时或错误，因为无法主动获取最新信息。这是由于当前语言模型仅依赖预训练数据，与外界“断开”。要克服这一缺陷， LangChain 框架提出了 “代理”(Agent) 的解决方案。代理作为语言模型的外部模块，可提供计算、逻辑、检索等功能的支持，使语言模型获得异常强大的推理和获取信息的超能力。

### 5.1 使用LangChain内置工具llm-math和wikipedia

要使用代理 (Agents) ，我们需要三样东西：

- 一个基本的 LLM
- 我们将要进行交互的工具 Tools
- 一个控制交互的代理 (Agents) 。

```python
from langchain.agents import load_tools, initialize_agent
from langchain.agents import AgentType
from langchain.python import PythonREPL
from langchain.chat_models import ChatOpenAI
```

首先，让我们新建一个基本的 LLM 

```python
# 参数temperature设置为0.0，从而减少生成答案的随机性。
llm = ChatOpenAI(temperature=0)
```

接下来，初始化`工具 Tool` ，我们可以创建自定义工具 Tool 或加载预构建工具 Tool。无论哪种情况，工具 Tool 都是一个给定工具 `名称 name` 和 `描述 description` 的 实用链。

- `llm-math` 工具结合语言模型和计算器用以进行数学计算
- `wikipedia`工具通过API连接到wikipedia进行搜索查询。


```python

tools = load_tools(
    ["llm-math","wikipedia"], 
    llm=llm #第一步初始化的模型
)
```

现在我们有了 LLM 和工具，最后让我们初始化一个简单的代理 (Agents) ：

```python
# 初始化代理
agent= initialize_agent(
    tools, #第二步加载的工具
    llm, #第一步初始化的模型
    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,  #代理类型
    handle_parsing_errors=True, #处理解析错误
    verbose = True #输出中间步骤
)
```

- `agent`: 代理类型。这里使用的是`AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION`。其中`CHAT`代表代理模型为针对对话优化的模型；`Zero-shot` 意味着代理 (Agents) 仅在当前操作上起作用，即它没有记忆；`REACT`代表针对REACT设计的提示模版。`DESCRIPTION`根据工具的描述 description 来决定使用哪个工具。(我们不会在本章中讨论 * REACT 框架 ，但您可以将其视为 LLM 可以循环进行 Reasoning 和 Action 步骤的过程。它启用了一个多步骤的过程来识别答案。)
- `handle_parsing_errors`: 是否处理解析错误。当发生解析错误时，将错误信息返回给大模型，让其进行纠正。
- `verbose`: 是否输出中间步骤结果。

### 5.1.1 使用代理回答数学问题


```python
agent("计算300的25%") 
```

    
    
    > Entering new AgentExecutor chain...
    Question: 计算300的25%
    Thought: I can use the calculator tool to calculate 25% of 300.
    Action:
    ```json
    {
      "action": "Calculator",
      "action_input": "300 * 0.25"
    }
    ```
    
    Observation: Answer: 75.0
    Thought:The calculator tool returned the answer 75.0, which is 25% of 300.
    Final Answer: 25% of 300 is 75.0.
    
    > Finished chain.





    {'input': '计算300的25%', 'output': '25% of 300 is 75.0.'}



**上面的过程可以总结为下**

1. 模型对于接下来需要做什么，给出思考
    
   <p style="font-family:verdana; font-size:12px;color:green"> <strong>思考</strong>：我可以使用计算工具来计算300的25%</p>

2. 模型基于思考采取行动
    <p style="font-family:verdana; font-size:12px;color:green"> <strong>行动</strong>: 使用计算器（calculator），输入（action_input）300*0.25</p>
3. 模型得到观察
    <p style="font-family:verdana; font-size:12px;color:green"><strong>观察</strong>：答案: 75.0</p>

4. 基于观察，模型对于接下来需要做什么，给出思考
    <p style="font-family:verdana; font-size:12px;color:green"> <strong>思考</strong>: 计算工具返回了300的25%，答案为75 </p>

5. 给出最终答案（Final Answer）
     <p style="font-family:verdana; font-size:12px;color:green"> <strong>最终答案</strong>: 300的25%等于75。 </p>
5. 以字典的形式给出最终答案。

### 5.1.2 使用代理进行外部搜索


```python
question = "Tom M. Mitchell是一位美国计算机科学家，\
也是卡内基梅隆大学（CMU）的创始人大学教授。\
他写了哪本书呢？"

agent(question) 
```

    
    
    > Entering new AgentExecutor chain...
    Thought: I can use Wikipedia to find information about Tom M. Mitchell and his books.
    Action:
    ```json
    {
      "action": "Wikipedia",
      "action_input": "Tom M. Mitchell"
    }
    ```
    Observation: Page: Tom M. Mitchell
    Summary: Tom Michael Mitchell (born August 9, 1951) is an American computer scientist and the Founders University Professor at Carnegie Mellon University (CMU). He is a founder and former Chair of the Machine Learning Department at CMU. Mitchell is known for his contributions to the advancement of machine learning, artificial intelligence, and cognitive neuroscience and is the author of the textbook Machine Learning. He is a member of the United States National Academy of Engineering since 2010. He is also a Fellow of the American Academy of Arts and Sciences, the American Association for the Advancement of Science and a Fellow and past President of the Association for the Advancement of Artificial Intelligence. In October 2018, Mitchell was appointed as the Interim Dean of the School of Computer Science at Carnegie Mellon.
    
    Page: Tom Mitchell (Australian footballer)
    Summary: Thomas Mitchell (born 31 May 1993) is a professional Australian rules footballer playing for the Collingwood Football Club in the Australian Football League (AFL). He previously played for the Adelaide Crows, Sydney Swans from 2012 to 2016, and the Hawthorn Football Club between 2017 and 2022. Mitchell won the Brownlow Medal as the league's best and fairest player in 2018 and set the record for the most disposals in a VFL/AFL match, accruing 54 in a game against Collingwood during that season.
    Thought:The book written by Tom M. Mitchell is "Machine Learning".
    Thought: I have found the answer.
    Final Answer: The book written by Tom M. Mitchell is "Machine Learning".
    
    > Finished chain.





    {'input': 'Tom M. Mitchell是一位美国计算机科学家，也是卡内基梅隆大学（CMU）的创始人大学教授。他写了哪本书呢？',
     'output': 'The book written by Tom M. Mitchell is "Machine Learning".'}



✅ **总结**

1. 模型对于接下来需要做什么，给出思考（Thought） 
   <p style="font-family:verdana; font-size:12px;color:green"> <strong>思考</strong>：我应该使用维基百科去搜索。</p>

2. 模型基于思考采取行动（Action）
    <p style="font-family:verdana; font-size:12px;color:green"> <strong>行动</strong>: 使用维基百科，输入Tom M. Mitchell</p>
3. 模型得到观察（Observation）
    <p style="font-family:verdana; font-size:12px;color:green"><strong>观测</strong>: 页面: Tom M. Mitchell，页面: Tom Mitchell (澳大利亚足球运动员)</p>

4. 基于观察，模型对于接下来需要做什么，给出思考（Thought）
    <p style="font-family:verdana; font-size:12px;color:green"> <strong>思考</strong>: Tom M. Mitchell写的书是Machine Learning </p>

5. 给出最终答案（Final Answer）
     <p style="font-family:verdana; font-size:12px;color:green"> <strong>最终答案</strong>: Machine Learning </p>
5. 以字典的形式给出最终答案。


值得注意的是，模型每次运行推理的过程可能存在差异，但最终的结果一致。